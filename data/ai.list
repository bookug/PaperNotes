---
id=
title=
author=
journal=
year=
tags=
star=
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=0
title=Learning Deep Generative Models of Graphs
author=Yujia Li
journal=arXiv
year=
tags=GNN(graph neural network), GCN(graph convolutional network), deep Generative model of graphs
star=***
problem=
interest=
hardness=
idea=can capture both their structure and attributes. Our approach uses graph neural networks to express probabilistic dependencies among a graph’s nodes and edges, and can, in principle, learn distributions over any arbitrary graph
future=
comment=
other=
---
id=1
title=GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
author=Jiayuan You
journal=ICML
year=2018
tags=graph RNN(recurrent neural network)
star=****
problem=
interest=
hardness=
idea=a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far
future=
comment=
other=
---
id=2
title=Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network  (DAM)
author=xiangyang zhou, Lu Li (baidu corporation)
journal=Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)  (ACL)
year=2018
tags=response Selection, chatbot, depp Attention matching network, dialogue, DAM, NLP, machine translation, attention mechanism, Retrieval-based
star=****
problem=investigate matching a response with its multi-turn context using dependency information based entirely on attention.
interest=Human generates responses relying on semantic and functional dependencies, including coreference relation, among dialogue elements and their context.
hardness=Early studies on response selection only use the last utterance in context for matching a reply, which is referred to as single-turn response selection;  Previous studies show that capturing those matched segment pairs at different granularities across context and response is the key to multiturn response selection; existing models only consider the textual relevance, which suffers from matching response that latently depends on previous turns
idea=construct representations of text segments at different granularities solely with stacked self-attention; extract the truly matched segment pairs with attention across the context and response; DAM takes each single word of an utterance in context or response as the centric-meaning of an abstractive semantic segment, and hierarchically enriches its representation with stacked self-attention, gradually producing more and more sophisticated segment representations surrounding the centric-word; self attention(different granularities, word, phrase, sentence) and cross-attention(interaction of sentences)
future=generating adversarial examples, rather than randomly sampling, during training procedure may be a good idea for addressing both fuzzy-candidate and logical-error, and to capture logic-level information hidden behind conversation text is also worthy to be studied in the future; We would like to explore in depth how attention can help improve neural dialogue modeling for both chatbots and taskoriented dialogue systems in our future work.
comment=
other=introduce attention mechanism into multi-turn response selection
---
id=3
title=Multi-view Response Selection for Human-Computer Conversation
author=xiangyang zhou, daxiang dong, rui yan(the teacher)
journal=EMNLP
year=2016
tags=response selection, dialogue, QA, NLP, multi-turn human-computer conversation, LSTM, RNN, DNN-based
star=***
problem=
interest=
hardness=Previous approaches take word as a unit and view context and response as sequences of words. This kind of approaches do not explicitly take each utterance as a unit, therefore it is difficult to catch utterancelevel discourse information and dependencies
idea=a multi-view response selection model that integrates information from two different views, i.e., word sequence view and utterance sequence view.  We jointly model the two views via deep neural networks
future=
comment=
other=
---
id=4
title=Multi-Representation Fusion Network for Multi-turn Response Selection in Retrieval-based Chatbots
author=chongyang tao, wei wu, wenpeng hu, dongyan zhao, rui yan
journal=WSDM(CCF B conference of data mining)
year=2019
tags=Multi-Representation fusion network, multi-response selection, Retrieval-based chatbot, NLP, QA
star=***
problem=context-response matching with multiple types of representations for multi-turn response selection in retrieval-based chatbots
interest=The representations encode semantics of contexts and responses on words, n-grams, and sub-sequences of utterances, and capture both short-term and long-term dependencies among words
hardness=
idea=With such a number of representations in hand, we study how to fuse them in a deep neural architecture for matching and how each of them contributes to matching. To this end, we propose a multirepresentation fusion network where the representations can be fused into matching at an early stage, at an intermediate stage, or at the last stage
future=
comment=Evaluation results indicate that late fusion is always better than early fusion, and by fusing the representations at the last stage, our model significantly outperforms the existing methods
other=introduce attention mechanism into SMN
---
id=4
title=attention is all you need
author=Ashish Vaswani (Google Brain, Google research)
journal=NIPS(Neural Information Processing Systems)
year=2017
tags=attention mechanism,
star=****
problem=
interest=
hardness=The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism
idea=a new simple sequence-to-sequence network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely
future=
comment=Not only Transformer can achieve better translation results than convenient RNN-based models, but also it is very fast in training/predicting as the computation of attention can be fully parallelized
other=
---
id=5
title=Distributed Representations of Words and Phrases and their Compositionality
author=Tomas Mikolov, Jeffrey Dean (Google Inc.)
journal=NIPS
year=2013
tags=word embedding, word vector, words and phrases
star=****
problem=several extensions that improve both the quality of the vectors and the training speed
interest=The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships
hardness=An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases
idea=By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling; present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible
future=
comment=
other=
---
id=6
title=Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots  (SMN)
author=Yu Wu, Wei Wu, Zhoujun Li(*)
journal=ACL
year=2017
tags=SMN, Sequential matching network, multi-turn response selection, Retrieval-Based, Chatbot, NLP
star=****
problem=response selection for multiturn conversation in retrieval-based chatbots
interest=Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among utterances or important contextual information
hardness=
idea=a sequential matching network (SMN) to address both problems.  SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations.  The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among utterances. The final matching score is calculated with the hidden states of the RNN
future=
comment=
other=
---
id=7
title=Reducing the Dimensionality with Neural Networks
author=GE Hinton
journal=science
year=2006
tags=neural network, Dimensionality reduction
star=****
problem=
interest=
hardness=
idea=reduce Dimensionality with deep multiple layers
future=
comment=
other=单层神经网络的缺点： 我们的优化方法不一定能够找到我们所希望的优化参数，也就找不到我们需要的拟合函数，虽然这个拟合的函数是存在的； 训练结果好，但是泛化能力差，即很容易产生过拟合;   https://blog.csdn.net/neu_dr1996/article/details/78689990
---
id=8
title=Approximation by Superpositions of a Sigmoidal Function
author=George Cybenko
journal=MCSS
year=1989
tags=Universal Approximation Properties,sigmoidal function,
star=****
problem=
interest=
hardness=
idea=
future=
comment=一个单隐藏层的神经网络，如果神经元个数足够多，通过非线性的激活函数则可以拟合任意函数。这使得我们在思考神经网络的问题的时候，不需要考虑函数是否能够用神经网络拟合，只需要考虑如何用神经网络做到更好的拟合。
other=
---
id=9
title=Multilayer feedforward networks are universal approximators
author=KurtHornik
journal=
year=1991
tags=Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks
star=****
problem=
interest=
hardness=
idea=
future=
comment=通用逼近属性并不是激活函数的具体选择，而是多层前馈结构本身，该结构使神经网络具有通用逼近器的性质。
other=
---
id=10
title=Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots (IMN)
author=
journal=arxiv
year=2019
tags=IMN, Retrieval-Based Chatbots, Multi-Turn Response selection, Interactive matching network
star=**
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=11
title=Learning Matching Models with Weak Supervision for Response Selection in Retrieval-based Chatbots
author=Yu Wu, Wei wu
journal=ACL
year=2018
tags=negative sampling, generative model, weak Supervision, response Selection, Retrieval-based chatbots
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=12
title=experiments on learning by back propagation
author=Geoffrey Hinton (AI, Turing award)
journal=
year=1986
tags=ai, back propagation, machine learning, deep learning
star=*****
problem=
interest=
hardness=
idea=
future=
comment=
other=AlexNet in ImageNet, 2012;   restricted Boltzmann machine, RBM
---
id=13
title=convolutional neural network
author=Yann LeCun (AI, Turing award)
journal=
year=1980
tags=cnn, ai, deep learning
star=*****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=14
title=a neural probabilistic language model
author=Yoshua Bengio  (AI, Turing award)
journal=
year=2000
tags=high-dimension word embedding, ai, probabilistic language model, NLP
star=*****
problem=
interest=
hardness=
idea=
future=
comment=
other=probabilistic model of sequences, attention mechanism, GAN
---
id=15
title=Deep Learning  (survey)
author=Geoffrey Hinton, Yann LeCun, Yoshua Bengio  (AI, all 2019 Turing awards, the first in AI area)
journal=Nature
year=2015
tags=survey,deep Learning, AI
star=*****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=16
title=Geometric deep learning: going beyond Euclidean data  (survey)
author=M. M. Bronstein
journal=IEEE Signal Processing Magazine
year=2017
tags=Geometric deep learning, GCN, Euclidean data, survey
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=17
title=Deep learning on graphs: a survey
author=Z. Zhang
journal=arXiv
year=2018
tags=survey, deep learning on graph, GCN
star=***
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=18
title=Graph Neural Networks: A Review of Methods and Applications (survey)
author=J. Zhou
journal=arXiv
year=2018
tags=GCN, graph neural Networks, survey
star=***
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=19
title=Representation Learning on Graphs: Methods and Applications (survey)
author=W. L. Hamilton
journal=arXiv
year=2017
tags=Representation Learning on graphs, GCN, survey
star=***
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=20
title=Learning Convolutional Neural Networks for Graphs
author=
journal=ICML
year=2016
tags=GCN, Convolutional neural Networks, Graphs
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=21
title=Graph Wavelet Neural Network
author=
journal=ICLR
year=2019
tags=GCN, Graph Wavelet Neural Network, Fourier transform
star=****
problem=
interest=
hardness=
idea=采用图小波变换代替图傅里叶变换实现信号从节点域到谱域的映射，在谱域定义卷积核实现参数共享，进而通过图小波逆变换得到卷积后的信号
future=
comment=
other=
---
id=22
title=Least squares support vector machine classifiers  (SVM)
author=J. A. Suykens
journal=Neural Processing Letters
year=1999
tags=SVM, support vector machine classifiers, ai, machine Learning
star=*****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=23
title=Soft margins for AdaBoost
author=
journal=Machine Learning
year=2001
tags=soft margins, AdaBoost, machine Learning
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=24
title=Joint embedding of words and labels for text classification (LEAM)
author=Guoyin Wang
journal=ACL
year=2018
tags=LEAM, text classification, machine Learning, nlp, word embedding, label embedding
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=25
title=Graph Matching Networks for Learning the Similarity of Graph Structured Objects
author=Yujia Li, Chenjie Gu (DeepMind), Thomas Dullien(Google)
journal=ICML
year=2019
tags=GMN, GCN, GNN, graph matching network, graph similarity
star=****
problem=This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions.
interest=
hardness=
idea=First, we demonstrate how Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning. Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism.
future=
comment=We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flowgraph based function similarity search that plays an important role in the detection of vulnerabilities in software systems. The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domainspecific baseline systems that have been carefully hand-engineered for these problems.
other=
---
id=26
title=Lattice CNNs for Matching Based Chinese Question Answering 
author=Yuxuan Lai, Yansong Feng, Xiaohan Yu, Zheng Wang, Kun Xu, Dongyan Zhao
journal=AAAI
year=2019
tags=Lattice CNN(LCN), Matching Based, Chinese Question Answering, QA, NLP
star=****
problem=Short text matching often faces the challenges that there are great word mismatch and expression diversity between the two texts, which would be further aggravated in languages like Chinese where there is no natural space to segment words explicitly
interest=
hardness=
idea= a novel lattice based CNN model (LCNs) to utilize multi-granularity information inherent in the word lattice while maintaining strong ability to deal with the introduced noisy information for matching based question answering in Chinese.
future=
comment=
other=https://github.com/Erutan-pku/LCN-for-Chinese-QA
---
id=27
title=M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network
author=Qijie Zhao, Yongtao Wang
journal=AAAI
year=2019
tags=M2Det, Single-Shot object Detector, Multi-Level Feature Pyramid Network, Computer Vision
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=28
title=The lack of a priori distinctions between learning algorithms
author=David H Wolpert
journal=Neural computation
year=1996
tags=machine learning algorithms, priori distinctions, No Free Lunch (NFL), deep learning, DNN
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=29
title=No free lunch theorems for search
author=David H Wolpert
journal=Technical Report
year=1995
tags=machine learning algorithms, No Free Lunch (NFL), deep learning, DNN
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=没有免费的午餐定理(No Free Lunch Theorem)告诉我们, 对于任意两个学习算法和, 如果在一些问题上面比更好, 那么一定存在另外一些问题使得比更好. 也就是说, 脱离具体问题, 空谈哪个算法更好是没有意义的, 因为如果考虑所有可能的问题, 那么所有的算法都一样好.                 神经网络由于能自动的学习合适的数据的表示, 因此更适合于图像, 语音, 文本这种数据的原始表示和数据的特征之间相差很大的情况, 这也是现今深度学习在这些领域能取得重大进展的重要原因. 而另一方面, 仍然有相当多的问题, 其数据的原始表示和数据的特征之间差别不大, 这个时候, 使用深度学习开销和性能将比不过经典方法.
---
id=30
title=Theoretical Impediments to Machine learning with seven Sparks from the Casual Revolution
author=Judea Pearl (father of Bayesian network, Turing Award)
journal=Arxiv
year=2018
tags=
star=****
problem=
interest=
hardness=
idea=the drawbacks of current machine learning works, strong AI should be based on Casual Revolution (因果革命)
future=
comment=
other=
---
id=31
title=ScalableGCN
author=
journal=
year=2019
tags=alibaba, scalable GCN, mini-batch GCN model, accelerative computing
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=https://github.com/alibaba/euler/wiki/ScalableGCN
---
id=32
title=Generalized Inverse Optimization through Online Learning
author=Chaoseng Dong, Yiran Chen, Bo Zeng
journal=NeurIPS (original NIPS conference)
year=2018
tags=Generalized inverse Optimization, online learning, matching learning
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=33
title=Open Problem: Is Margin Sufficient for Non-Interactive Private Distributed Learning
author=Amit Daniely, Vitaly Feldman
journal=COLT (CCF B conference of AI theory, but very good)
year=2019
tags=open Problem, margin, Non-Interactive Private Distributed Learning
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=34
title=Open Problem: Monotonicity of Learning
author=Tom J. Viering
journal=COLT (CCF B conference of AI theory, but very good)
year=2019
tags=open Problem, Monotonicity, learning, Finite sample behavior, Learnability, Monotonic learning
star=****
problem=We pose the question to what extent a learning algorithm behaves monotonically in the following sense: does it perform better, in expectation, when adding one instance to the training set?
interest=
hardness=
idea=We focus on empirical risk minimization and illustrate this property with several examples, two where it does hold and two where it does not. We also relate it to the notion of PAC-learnability.
future=
comment=
other=
---
id=35
title=Open Problem: The Oracle Complexity of Convex Optimization with Limited Memory
author=Blake E. Woodworth, Nathan Srebro
journal=COLT (CCF B conference of AI theory, but very good)
year=2019
tags=open Problem, Oracle Complexity, first order convex Optimization, limited memory
star=****
problem=We note that known methods achieving the optimal oracle complexity for first order convex optimization require quadratic memory, and ask whether this is necessary, and more broadly seek to characterize the minimax number of first order queries required to optimize a convex Lipschitz function subject to a memory constraint.
interest=
hardness=
idea=
future=
comment=
other=
---
id=36
title=Inference under Information Constraints: Lower Bounds from Chi-Square Contraction
author=Jayadev Acharya, Clément L. Canonne, Himanshu Tyagi
journal=COLT (CCF B conference of AI theory, but very good), arXiv
year=2019
tags=Inference, Information Constraints, lower bound, Chi-Square Contraction
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=37
title=GraphSAINT: Graph Sampling Based Inductive Learning Method 
author=
journal=ICLR  openreview
year=2020
tags=GCN, AI, graph mining, GraphSAINT, graph Sampling, Inductive Learning method, GraphSAGE
star=***
problem=这篇文章的思路非常新颖，它解决的问题是，本来我们训练GCN，为了方便和效率起见，我们一般是采样每个顶点的固定大小邻域 (Hamilton et al 2017 GraphSAGE)，这样每个顶点的邻居都是规则的，复杂度可控的。虽然还有很多采样方法 (Huang et al 2018, Chen et al 2018 FastGCN, Li et al 2019 GCN-LASE)，但是根本上还是对每个顶点采样其邻居。这就导致一个问题，就是我们不能采样特别深的GCN，因为一旦深就指数爆炸了，比如说每个节点采样20个邻居，两层就是400个，四层就是160000个（但是GCN不是一旦超过4层就会过度平滑导致完全失去作用吗？有印象是这么说的，求指正）。
interest=
hardness=
idea=本文要解决的就是这个问题，它从根本上改变了采样方式。它不再在原图上采样邻居，而是从原图上采样一个子图下来，在这个子图上做局部的、完全展开的GCN。这样做的好处是，不仅我们解决了邻居采样的指数化问题，而且我们可以对采下来的子图进行直接的并行化，就大大的改进了效率。当然，采样子图肯定不是随便采的，直观来说，影响大的节点肯定倾向于要被保留下来，但这就引入了采样的bias。为了解决这个问题，本文作者采用了改进的聚合，在聚合中引入被采样的概率，使得整体的卷积层计算还是无偏的。
future=
comment=实验结果看起来很牛逼，但我没有想通为什么它能吊锤GCN和GraphSAGE那么多，在此抛砖引玉等一个解释吧。曹恭泽 的解释，GCN相当于full gradient descent，没有batch泛化好。GraphSAGE采样batch方差大，收敛性不好。我觉得有道理。这篇文章改变了我们一直以来做GCN的采样思路，也非常有新意。
other=https://openreview.net/forum?id=BJe8pkHFwS
---
id=38
title=Demystifying Graph Neural Network Via Graph Filter Assessment  (Adaptive Filter Graph Neural Network (AFGNN))
author=
journal=ICLR  openreview
year=2020
tags=GCN, AI, graph mining, graph Neural Network, graph Filter Assessment, Adaptive Filter Graph Neural Network (AFGNN), task-specific filter
star=***
problem=
interest=
hardness=
idea=这篇文章从LDA（线性判别分析）借鉴了灵感，通过刻画不同类别的图节点是否能够通过卷积核变化后线性可分，来评价图卷积核的好坏。此外，作者还探究了最优图卷积核与原图性质的关系，我觉得让我眼前一亮的是，作者发现图卷积核的好坏核图度数的power law系数有关。这是一个很有趣的性质，我猜测是因为不同power law系数的图会有不一样的结构特征（比如有不一样的graphlet分布，聚集系数etc）。
future=
comment=
other=https://openreview.net/forum?id=r1erNxBtwr
---
id=39
title=Learning What and Where to Transfer
author=Yunhun Jang, Hankook Lee, Sung Ju Hwang, Jinwoo Shin
journal=ICML
year=2019
tags=machine Learning, meta-networks, Transfer learning, heterogeneous network
star=****
problem=使用 meta-networks 在异构网络之间完成更加准确有效的知识迁移，即在目标模型和源模型之间，哪些层对的哪些特征进行多大程度的知识迁移。通过实验论证了知识从大网络迁移到小网络，从而增强小网络的学习表现的可行性，诚如作者所言：我们的工作将为异构或/和多个网络架构和任务之间的复杂转移学习任务提供一个新的视角。
interest=
hardness=
idea=我们的目标是学习将有用的知识从源网络转移到目标网络，而不需要手动关联层或选择特性。为此,我们提出一个元学习方法（使用元神经网络参数 [公式] 来学习目标模型的参数 [公式] ），学习哪些知识的源网络传输层的目标网络。在本文中，我们主要关注卷积神经网络之间的转移学习，但是我们的方法是通用的，也适用于其他类型的深度神经网络。
future=
comment=
other=https://github.com/alinlab/L2T-ww   https://zhuanlan.zhihu.com/p/84926780
---
