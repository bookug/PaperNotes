---
id=
title=
author=
journal=
year=
tags=
star=
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=0
title=Learning Deep Generative Models of Graphs
author=Yujia Li
journal=arXiv
year=
tags=GNN(graph neural network), GCN(graph convolutional network), deep Generative model of graphs
star=***
problem=
interest=
hardness=
idea=can capture both their structure and attributes. Our approach uses graph neural networks to express probabilistic dependencies among a graph’s nodes and edges, and can, in principle, learn distributions over any arbitrary graph
future=
comment=
other=
---
id=1
title=GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
author=Jiayuan You
journal=ICML
year=2018
tags=graph RNN(recurrent neural network)
star=****
problem=
interest=
hardness=
idea=a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far
future=
comment=
other=
---
id=2
title=Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network  (DAM)
author=xiangyang zhou, Lu Li (baidu corporation)
journal=Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)  (ACL)
year=2018
tags=response Selection, chatbot, depp Attention matching network, dialogue, DAM, NLP, machine translation, attention mechanism, Retrieval-based
star=****
problem=investigate matching a response with its multi-turn context using dependency information based entirely on attention.
interest=Human generates responses relying on semantic and functional dependencies, including coreference relation, among dialogue elements and their context.
hardness=Early studies on response selection only use the last utterance in context for matching a reply, which is referred to as single-turn response selection;  Previous studies show that capturing those matched segment pairs at different granularities across context and response is the key to multiturn response selection; existing models only consider the textual relevance, which suffers from matching response that latently depends on previous turns
idea=construct representations of text segments at different granularities solely with stacked self-attention; extract the truly matched segment pairs with attention across the context and response; DAM takes each single word of an utterance in context or response as the centric-meaning of an abstractive semantic segment, and hierarchically enriches its representation with stacked self-attention, gradually producing more and more sophisticated segment representations surrounding the centric-word; self attention(different granularities, word, phrase, sentence) and cross-attention(interaction of sentences)
future=generating adversarial examples, rather than randomly sampling, during training procedure may be a good idea for addressing both fuzzy-candidate and logical-error, and to capture logic-level information hidden behind conversation text is also worthy to be studied in the future; We would like to explore in depth how attention can help improve neural dialogue modeling for both chatbots and taskoriented dialogue systems in our future work.
comment=
other=introduce attention mechanism into multi-turn response selection
---
id=3
title=Multi-view Response Selection for Human-Computer Conversation
author=xiangyang zhou, daxiang dong, rui yan(the teacher)
journal=EMNLP
year=2016
tags=response selection, dialogue, QA, NLP, multi-turn human-computer conversation, LSTM, RNN, DNN-based
star=***
problem=
interest=
hardness=Previous approaches take word as a unit and view context and response as sequences of words. This kind of approaches do not explicitly take each utterance as a unit, therefore it is difficult to catch utterancelevel discourse information and dependencies
idea=a multi-view response selection model that integrates information from two different views, i.e., word sequence view and utterance sequence view.  We jointly model the two views via deep neural networks
future=
comment=
other=
---
id=4
title=Multi-Representation Fusion Network for Multi-turn Response Selection in Retrieval-based Chatbots
author=chongyang tao, wei wu, wenpeng hu, dongyan zhao, rui yan
journal=WSDM(CCF B conference of data mining)
year=2019
tags=Multi-Representation fusion network, multi-response selection, Retrieval-based chatbot, NLP, QA
star=***
problem=context-response matching with multiple types of representations for multi-turn response selection in retrieval-based chatbots
interest=The representations encode semantics of contexts and responses on words, n-grams, and sub-sequences of utterances, and capture both short-term and long-term dependencies among words
hardness=
idea=With such a number of representations in hand, we study how to fuse them in a deep neural architecture for matching and how each of them contributes to matching. To this end, we propose a multirepresentation fusion network where the representations can be fused into matching at an early stage, at an intermediate stage, or at the last stage
future=
comment=Evaluation results indicate that late fusion is always better than early fusion, and by fusing the representations at the last stage, our model significantly outperforms the existing methods
other=introduce attention mechanism into SMN
---
id=4
title=attention is all you need
author=Ashish Vaswani (Google Brain, Google research)
journal=NIPS(Neural Information Processing Systems)
year=2017
tags=attention mechanism,
star=****
problem=
interest=
hardness=The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism
idea=a new simple sequence-to-sequence network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely
future=
comment=Not only Transformer can achieve better translation results than convenient RNN-based models, but also it is very fast in training/predicting as the computation of attention can be fully parallelized
other=
---
id=5
title=Distributed Representations of Words and Phrases and their Compositionality
author=Tomas Mikolov, Jeffrey Dean (Google Inc.)
journal=NIPS
year=2013
tags=word embedding, word vector, words and phrases
star=****
problem=several extensions that improve both the quality of the vectors and the training speed
interest=The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships
hardness=An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases
idea=By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling; present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible
future=
comment=
other=
---
id=6
title=Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots  (SMN)
author=Yu Wu, Wei Wu, Zhoujun Li(*)
journal=ACL
year=2017
tags=SMN, Sequential matching network, multi-turn response selection, Retrieval-Based, Chatbot, NLP
star=****
problem=response selection for multiturn conversation in retrieval-based chatbots
interest=Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among utterances or important contextual information
hardness=
idea=a sequential matching network (SMN) to address both problems.  SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations.  The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among utterances. The final matching score is calculated with the hidden states of the RNN
future=
comment=
other=
---
id=7
title=Reducing the Dimensionality with Neural Networks
author=GE Hinton
journal=science
year=2006
tags=neural network, Dimensionality reduction
star=****
problem=
interest=
hardness=
idea=reduce Dimensionality with deep multiple layers
future=
comment=
other=单层神经网络的缺点： 我们的优化方法不一定能够找到我们所希望的优化参数，也就找不到我们需要的拟合函数，虽然这个拟合的函数是存在的； 训练结果好，但是泛化能力差，即很容易产生过拟合;   https://blog.csdn.net/neu_dr1996/article/details/78689990
---
id=8
title=Approximation by Superpositions of a Sigmoidal Function
author=George Cybenko
journal=MCSS
year=1989
tags=Universal Approximation Properties,sigmoidal function,
star=****
problem=
interest=
hardness=
idea=
future=
comment=一个单隐藏层的神经网络，如果神经元个数足够多，通过非线性的激活函数则可以拟合任意函数。这使得我们在思考神经网络的问题的时候，不需要考虑函数是否能够用神经网络拟合，只需要考虑如何用神经网络做到更好的拟合。
other=
---
id=9
title=Multilayer feedforward networks are universal approximators
author=KurtHornik
journal=
year=1991
tags=Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks
star=****
problem=
interest=
hardness=
idea=
future=
comment=通用逼近属性并不是激活函数的具体选择，而是多层前馈结构本身，该结构使神经网络具有通用逼近器的性质。
other=
---
