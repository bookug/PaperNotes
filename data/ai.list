---
id=
title=
author=
journal=
year=
tags=
star=
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=0
title=Learning Deep Generative Models of Graphs
author=Yujia Li
journal=arXiv
year=
tags=GNN(graph neural network), GCN(graph convolutional network), deep Generative model of graphs
star=***
problem=
interest=
hardness=
idea=can capture both their structure and attributes. Our approach uses graph neural networks to express probabilistic dependencies among a graph’s nodes and edges, and can, in principle, learn distributions over any arbitrary graph
future=
comment=
other=
---
id=1
title=GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
author=Jiayuan You
journal=ICML
year=2018
tags=graph RNN(recurrent neural network)
star=****
problem=
interest=
hardness=
idea=a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far
future=
comment=
other=
---
id=2
title=Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network  (DAM)
author=xiangyang zhou, Lu Li (baidu corporation)
journal=Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)  (ACL)
year=2018
tags=response Selection, chatbot, depp Attention matching network, dialogue, DAM, NLP, machine translation, attention mechanism, Retrieval-based
star=****
problem=investigate matching a response with its multi-turn context using dependency information based entirely on attention.
interest=Human generates responses relying on semantic and functional dependencies, including coreference relation, among dialogue elements and their context.
hardness=Early studies on response selection only use the last utterance in context for matching a reply, which is referred to as single-turn response selection;  Previous studies show that capturing those matched segment pairs at different granularities across context and response is the key to multiturn response selection; existing models only consider the textual relevance, which suffers from matching response that latently depends on previous turns
idea=construct representations of text segments at different granularities solely with stacked self-attention; extract the truly matched segment pairs with attention across the context and response; DAM takes each single word of an utterance in context or response as the centric-meaning of an abstractive semantic segment, and hierarchically enriches its representation with stacked self-attention, gradually producing more and more sophisticated segment representations surrounding the centric-word; self attention(different granularities, word, phrase, sentence) and cross-attention(interaction of sentences)
future=generating adversarial examples, rather than randomly sampling, during training procedure may be a good idea for addressing both fuzzy-candidate and logical-error, and to capture logic-level information hidden behind conversation text is also worthy to be studied in the future; We would like to explore in depth how attention can help improve neural dialogue modeling for both chatbots and taskoriented dialogue systems in our future work.
comment=
other=introduce attention mechanism into multi-turn response selection
---
id=3
title=Multi-view Response Selection for Human-Computer Conversation
author=xiangyang zhou, daxiang dong, rui yan(the teacher)
journal=EMNLP
year=2016
tags=response selection, dialogue, QA, NLP, multi-turn human-computer conversation, LSTM, RNN, DNN-based
star=***
problem=
interest=
hardness=Previous approaches take word as a unit and view context and response as sequences of words. This kind of approaches do not explicitly take each utterance as a unit, therefore it is difficult to catch utterancelevel discourse information and dependencies
idea=a multi-view response selection model that integrates information from two different views, i.e., word sequence view and utterance sequence view.  We jointly model the two views via deep neural networks
future=
comment=
other=
---
id=4
title=Multi-Representation Fusion Network for Multi-turn Response Selection in Retrieval-based Chatbots
author=chongyang tao, wei wu, wenpeng hu, dongyan zhao, rui yan
journal=WSDM(CCF B conference of data mining)
year=2019
tags=Multi-Representation fusion network, multi-response selection, Retrieval-based chatbot, NLP, QA
star=***
problem=context-response matching with multiple types of representations for multi-turn response selection in retrieval-based chatbots
interest=The representations encode semantics of contexts and responses on words, n-grams, and sub-sequences of utterances, and capture both short-term and long-term dependencies among words
hardness=
idea=With such a number of representations in hand, we study how to fuse them in a deep neural architecture for matching and how each of them contributes to matching. To this end, we propose a multirepresentation fusion network where the representations can be fused into matching at an early stage, at an intermediate stage, or at the last stage
future=
comment=Evaluation results indicate that late fusion is always better than early fusion, and by fusing the representations at the last stage, our model significantly outperforms the existing methods
other=introduce attention mechanism into SMN
---
id=4
title=attention is all you need
author=Ashish Vaswani (Google Brain, Google research)
journal=NIPS(Neural Information Processing Systems)
year=2017
tags=attention mechanism,
star=****
problem=
interest=
hardness=The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism
idea=a new simple sequence-to-sequence network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely
future=
comment=Not only Transformer can achieve better translation results than convenient RNN-based models, but also it is very fast in training/predicting as the computation of attention can be fully parallelized
other=
---
id=5
title=Distributed Representations of Words and Phrases and their Compositionality
author=Tomas Mikolov, Jeffrey Dean (Google Inc.)
journal=NIPS
year=2013
tags=word embedding, word vector, words and phrases
star=****
problem=several extensions that improve both the quality of the vectors and the training speed
interest=The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships
hardness=An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases
idea=By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling; present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible
future=
comment=
other=
---
id=6
title=Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots  (SMN)
author=Yu Wu, Wei Wu, Zhoujun Li(*)
journal=ACL
year=2017
tags=SMN, Sequential matching network, multi-turn response selection, Retrieval-Based, Chatbot, NLP
star=****
problem=response selection for multiturn conversation in retrieval-based chatbots
interest=Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among utterances or important contextual information
hardness=
idea=a sequential matching network (SMN) to address both problems.  SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations.  The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among utterances. The final matching score is calculated with the hidden states of the RNN
future=
comment=
other=
---
id=7
title=Reducing the Dimensionality with Neural Networks
author=GE Hinton
journal=science
year=2006
tags=neural network, Dimensionality reduction
star=****
problem=
interest=
hardness=
idea=reduce Dimensionality with deep multiple layers
future=
comment=
other=单层神经网络的缺点： 我们的优化方法不一定能够找到我们所希望的优化参数，也就找不到我们需要的拟合函数，虽然这个拟合的函数是存在的； 训练结果好，但是泛化能力差，即很容易产生过拟合;   https://blog.csdn.net/neu_dr1996/article/details/78689990
---
id=8
title=Approximation by Superpositions of a Sigmoidal Function
author=George Cybenko
journal=MCSS
year=1989
tags=Universal Approximation Properties,sigmoidal function,
star=****
problem=
interest=
hardness=
idea=
future=
comment=一个单隐藏层的神经网络，如果神经元个数足够多，通过非线性的激活函数则可以拟合任意函数。这使得我们在思考神经网络的问题的时候，不需要考虑函数是否能够用神经网络拟合，只需要考虑如何用神经网络做到更好的拟合。
other=
---
id=9
title=Multilayer feedforward networks are universal approximators
author=KurtHornik
journal=
year=1991
tags=Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks
star=****
problem=
interest=
hardness=
idea=
future=
comment=通用逼近属性并不是激活函数的具体选择，而是多层前馈结构本身，该结构使神经网络具有通用逼近器的性质。
other=
---
id=10
title=Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots (IMN)
author=
journal=arxiv
year=2019
tags=IMN, Retrieval-Based Chatbots, Multi-Turn Response selection, Interactive matching network
star=**
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=11
title=Learning Matching Models with Weak Supervision for Response Selection in Retrieval-based Chatbots
author=Yu Wu, Wei wu
journal=ACL
year=2018
tags=negative sampling, generative model, weak Supervision, response Selection, Retrieval-based chatbots
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=12
title=experiments on learning by back propagation
author=Geoffrey Hinton (AI, Turing award)
journal=
year=1986
tags=ai, back propagation, machine learning, deep learning
star=*****
problem=
interest=
hardness=
idea=
future=
comment=
other=AlexNet in ImageNet, 2012;   restricted Boltzmann machine, RBM
---
id=13
title=convolutional neural network
author=Yann LeCun (AI, Turing award)
journal=
year=1980
tags=cnn, ai, deep learning
star=*****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=14
title=a neural probabilistic language model
author=Yoshua Bengio  (AI, Turing award)
journal=
year=2000
tags=high-dimension word embedding, ai, probabilistic language model, NLP
star=*****
problem=
interest=
hardness=
idea=
future=
comment=
other=probabilistic model of sequences, attention mechanism, GAN
---
id=15
title=Deep Learning  (survey)
author=Geoffrey Hinton, Yann LeCun, Yoshua Bengio  (AI, all 2019 Turing awards, the first in AI area)
journal=Nature
year=2015
tags=survey,deep Learning, AI
star=*****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=16
title=Geometric deep learning: going beyond Euclidean data  (survey)
author=M. M. Bronstein
journal=IEEE Signal Processing Magazine
year=2017
tags=Geometric deep learning, GCN, Euclidean data, survey
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=17
title=Deep learning on graphs: a survey
author=Z. Zhang
journal=arXiv
year=2018
tags=survey, deep learning on graph, GCN
star=***
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=18
title=Graph Neural Networks: A Review of Methods and Applications (survey)
author=J. Zhou
journal=arXiv
year=2018
tags=GCN, graph neural Networks, survey
star=***
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=19
title=Representation Learning on Graphs: Methods and Applications (survey)
author=W. L. Hamilton
journal=arXiv
year=2017
tags=Representation Learning on graphs, GCN, survey
star=***
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=20
title=Learning Convolutional Neural Networks for Graphs
author=
journal=ICML
year=2016
tags=GCN, Convolutional neural Networks, Graphs
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=21
title=Graph Wavelet Neural Network
author=
journal=ICLR
year=2019
tags=GCN, Graph Wavelet Neural Network, Fourier transform
star=****
problem=
interest=
hardness=
idea=采用图小波变换代替图傅里叶变换实现信号从节点域到谱域的映射，在谱域定义卷积核实现参数共享，进而通过图小波逆变换得到卷积后的信号
future=
comment=
other=
---
id=22
title=Least squares support vector machine classifiers  (SVM)
author=J. A. Suykens
journal=Neural Processing Letters
year=1999
tags=SVM, support vector machine classifiers, ai, machine Learning
star=*****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=23
title=Soft margins for AdaBoost
author=
journal=Machine Learning
year=2001
tags=soft margins, AdaBoost, machine Learning
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=24
title=Joint embedding of words and labels for text classification (LEAM)
author=Guoyin Wang
journal=ACL
year=2018
tags=LEAM, text classification, machine Learning, nlp, word embedding, label embedding
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=25
title=Graph Matching Networks for Learning the Similarity of Graph Structured Objects
author=Yujia Li, Chenjie Gu (DeepMind), Thomas Dullien(Google)
journal=ICML
year=2019
tags=GMN, GCN, GNN, graph matching network, graph similarity
star=****
problem=This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions.
interest=
hardness=
idea=First, we demonstrate how Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning. Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism.
future=
comment=We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flowgraph based function similarity search that plays an important role in the detection of vulnerabilities in software systems. The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domainspecific baseline systems that have been carefully hand-engineered for these problems.
other=
---
id=26
title=Lattice CNNs for Matching Based Chinese Question Answering 
author=Yuxuan Lai, Yansong Feng, Xiaohan Yu, Zheng Wang, Kun Xu, Dongyan Zhao
journal=AAAI
year=2019
tags=Lattice CNN(LCN), Matching Based, Chinese Question Answering, QA, NLP
star=****
problem=Short text matching often faces the challenges that there are great word mismatch and expression diversity between the two texts, which would be further aggravated in languages like Chinese where there is no natural space to segment words explicitly
interest=
hardness=
idea= a novel lattice based CNN model (LCNs) to utilize multi-granularity information inherent in the word lattice while maintaining strong ability to deal with the introduced noisy information for matching based question answering in Chinese.
future=
comment=
other=https://github.com/Erutan-pku/LCN-for-Chinese-QA
---
id=27
title=M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network
author=Qijie Zhao, Yongtao Wang
journal=AAAI
year=2019
tags=M2Det, Single-Shot object Detector, Multi-Level Feature Pyramid Network, Computer Vision
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=28
title=The lack of a priori distinctions between learning algorithms
author=David H Wolpert
journal=Neural computation
year=1996
tags=machine learning algorithms, priori distinctions, No Free Lunch (NFL), deep learning, DNN
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=29
title=No free lunch theorems for search
author=David H Wolpert
journal=Technical Report
year=1995
tags=machine learning algorithms, No Free Lunch (NFL), deep learning, DNN
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=没有免费的午餐定理(No Free Lunch Theorem)告诉我们, 对于任意两个学习算法和, 如果在一些问题上面比更好, 那么一定存在另外一些问题使得比更好. 也就是说, 脱离具体问题, 空谈哪个算法更好是没有意义的, 因为如果考虑所有可能的问题, 那么所有的算法都一样好.                 神经网络由于能自动的学习合适的数据的表示, 因此更适合于图像, 语音, 文本这种数据的原始表示和数据的特征之间相差很大的情况, 这也是现今深度学习在这些领域能取得重大进展的重要原因. 而另一方面, 仍然有相当多的问题, 其数据的原始表示和数据的特征之间差别不大, 这个时候, 使用深度学习开销和性能将比不过经典方法.
---
id=30
title=Theoretical Impediments to Machine learning with seven Sparks from the Casual Revolution
author=Judea Pearl (father of Bayesian network, Turing Award)
journal=Arxiv
year=2018
tags=
star=****
problem=
interest=
hardness=
idea=the drawbacks of current machine learning works, strong AI should be based on Casual Revolution (因果革命)
future=
comment=
other=
---
id=31
title=ScalableGCN
author=
journal=
year=2019
tags=alibaba, scalable GCN, mini-batch GCN model, accelerative computing
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=https://github.com/alibaba/euler/wiki/ScalableGCN
---
id=32
title=Generalized Inverse Optimization through Online Learning
author=Chaoseng Dong, Yiran Chen, Bo Zeng
journal=NeurIPS (original NIPS conference)
year=2018
tags=Generalized inverse Optimization, online learning, matching learning
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=33
title=Open Problem: Is Margin Sufficient for Non-Interactive Private Distributed Learning
author=Amit Daniely, Vitaly Feldman
journal=COLT (CCF B conference of AI theory, but very good)
year=2019
tags=open Problem, margin, Non-Interactive Private Distributed Learning
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=34
title=Open Problem: Monotonicity of Learning
author=Tom J. Viering
journal=COLT (CCF B conference of AI theory, but very good)
year=2019
tags=open Problem, Monotonicity, learning, Finite sample behavior, Learnability, Monotonic learning
star=****
problem=We pose the question to what extent a learning algorithm behaves monotonically in the following sense: does it perform better, in expectation, when adding one instance to the training set?
interest=
hardness=
idea=We focus on empirical risk minimization and illustrate this property with several examples, two where it does hold and two where it does not. We also relate it to the notion of PAC-learnability.
future=
comment=
other=
---
id=35
title=Open Problem: The Oracle Complexity of Convex Optimization with Limited Memory
author=Blake E. Woodworth, Nathan Srebro
journal=COLT (CCF B conference of AI theory, but very good)
year=2019
tags=open Problem, Oracle Complexity, first order convex Optimization, limited memory
star=****
problem=We note that known methods achieving the optimal oracle complexity for first order convex optimization require quadratic memory, and ask whether this is necessary, and more broadly seek to characterize the minimax number of first order queries required to optimize a convex Lipschitz function subject to a memory constraint.
interest=
hardness=
idea=
future=
comment=
other=
---
id=36
title=Inference under Information Constraints: Lower Bounds from Chi-Square Contraction
author=Jayadev Acharya, Clément L. Canonne, Himanshu Tyagi
journal=COLT (CCF B conference of AI theory, but very good), arXiv
year=2019
tags=Inference, Information Constraints, lower bound, Chi-Square Contraction
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
