---
id=
title=
author=
journal=
year=
tags=
star=
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=0
title=SKYLADDER: BETTER AND FASTER PRETRAINING VIA CONTEXT WINDOW SCHEDULING
author=Tongyao Zhu, Qian Liu, Haonan Wang, Shiqi Chen, Xiangming Gu, Tianyu Pang, Min-Yen Kan
journal=arXiv
year=2025
tags=MLsys, LLM infra, SKYLADDER, pretraining, dense model, context window schedule, long sequence
star=***
problem=models pretrained with shorter context windows consistently outperform their long-context counterparts under a fixed token budget
interest=Recent advancements in LLM pretraining have featured ever-expanding context windows to process longer sequences
hardness=long context not only harm the model performance, but also has higher traning costs
idea=explore an optimal context window scheduling strategy to better balance long-context capability with pretraining efficiency; SkyLadder, a simple yet effective approach that implements a short-to-long context window transition.
future=more dynamic adaptive scheduling for different model size and datasets, and MoE models, continue training and RL tasks, loss-based cutting
comment=SkyLadder preserves strong standard benchmark performance, while matching or exceeding baseline results on long-context tasks. Through extensive experiments, we pre-train 1B-parameter models (up to 32K context) and 3B-parameter models(8K context) on 100B tokens, demonstrating that SkyLadder yields consistent gains of up to 3.7% on common benchmarks, while achieving up to 22% faster training speeds compared to baselines
other=https://github.com/sail-sg/SkyLadder
---
id=1
title=Mirage: A Multi-Level Superoptimizer for Tensor Programs
author=Mengdi Wu, Xinhao Cheng, Shengyu Liu, Chunan Shi, Jianan Ji, Man Kit Ao, Praveen Velliengiri, Xupeng Miao, Oded Padon, Zhihao Jia
journal=OSDI
year=2025
tags=optimizer, tensor program, Mirage Persistent Kernel (MPK), LLM compiler, automated kernel generation, MegaKernel, uGraph, GPU compute hierarchy, compute graph, hierarchical graph representation, algebraic transformation, schedule transformation
star=****
problem=how to automatically generate big kernel containing all operations including communication
interest=codesign of algorithm and schedule; an algorithm describes what to compute in a kernel and a schedule specifies how to compute the kernel; Mirage automatically discovers and verifies sophisticated optimizations of tensor programs that require joint optimization of algebraic transformations, schedule transformations, and the discovery of new custom kernels
hardness=optimizes the schedule of a tensor program while fixing the algorithm. For a given algorithm, these optimizers automatically generate performant kernels by searching for possible strategies to execute the kernel on the target hardware. However, due to the linear algebra nature of DNNs, a tensor program can be represented by a wide spectrum of mathematically equivalent algorithms. Existing schedule-based optimizers only consider kernels whose algorithms are manually specified by users, resulting in missed optimization opportunities
idea=A key idea in Mirage is µGraphs, a uniform representation of tensor programs at the kernel, thread block, and thread levels of the GPU compute hierarchy. µGraphs enable Mirage to discover novel optimizations that combine algebraic transformations, schedule transformations, and generation of new custom kernels. To navigate the large search space, Mirage introduces a pruning technique based on abstraction that significantly reduces the search space and provides a certain optimality guarantee. To ensure that the optimized µGraph is equivalent to the input program, Mirage introduces a probabilistic equivalence verification procedure with strong theoretical guarantees
future=
comment= Mirage outperforms existing tensor program optimizers by up to 3.3×, even for widely used and heavily optimized DNNs ;  currently not support MoE well; MegaKernel of stanford is manually written for 1B model and single card
other=https://github.com/mirage-project/mirage/tree/mpk  ;  https://www.zhihu.com/question/1927927257713849225/answer/1928107746865185836?share_code=gjzLEtbuTe97&utm_psn=1930943423210329801
---
id=2
title=Task-Based Tensor Computations on Modern GPUs
author=ROHAN YADAV, MICHAEL GARLAND, ALEX AIKEN, MICHAEL BAUER
journal=PLDI
year=2025
tags=NVIDIA Hopper GPU, task-based tensor Computation, fixed-function unit, domain-specific, asynchronous data movement unit (TMA), asynchronous matrix multiplication unit (tensor core), warp-specialized kernel, producer-consumer pipeline, Cypress, programming model, compiler architecture, CUDA program
star=****
problem=Domain-specific, fixed-function units are becoming increasingly common in modern processors
interest=As the computational demands of applications evolve, the capabilities and programming interfaces of these fixed-function units continue to change. NVIDIA’s Hopper GPU architecture contains multiple fixed-function units per compute unit, including an asynchronous data movement unit (TMA) and an asynchronous matrix multiplication unit (Tensor Core).
hardness=Efficiently utilizing these units requires a fundamentally different programming style than previous architectures; programmers must now develop warp-specialized kernels that orchestrate producerconsumer pipelines between the asynchronous units
idea=To manage the complexity of programming these new architectures, we introduce Cypress, a task-based programming model with sequential semantics. Cypress programs are a set of designated functions called tasks that operate on tensors and are free of communication and synchronization. Cypress programs are bound to the target machine through a mapping specification that describes where tasks should run and in which memories tensors should be materialized. We present a compiler architecture that lowers Cypress programs into CUDA programs that perform competitively with expert-written codes
future=
comment=Cypress achieves 0.88x-1.06x the performance of cuBLAS on GEMM, and between 0.80x-0.98x the performance of the currently best-known Flash Attention implementation while eliminating all aspects of explicit data movement and asynchronous computation from application code
other=
---
id=3
title=Dynamic Inter-Thread Vectorization Architecture: extracting DLP from TLP
author=Sajith Kalathingal, Caroline Collange, Bharath Narasimha Swamy, André Seznec, Bharath N Swamy
journal=SBAC-PAD
year=2016
tags=DITVA, SPMD, DLP, TLP, SMT Architecture, AVX instruction, dynamic vector instruction
star=***
problem=generalize the successful SIMT GPU execution model to general-purpose SMT CPUs
interest=Threads of Single-Program Multiple-Data (SPMD) applications often execute the same instructions on different data.
hardness=
idea=propose the Dynamic Inter-Thread Vectorization Architecture (DITVA) to leverage this implicit data-level parallelism in SPMD applications by assembling dynamic vector instructions at runtime. DITVA extends an SIMD-enabled in-order SMT processor with an inter-thread vectorization execution mode. In this mode, multiple scalar threads running in lockstep share a single instruction stream and their respective instruction instances are aggregated into SIMD instructions. To balance thread- and data-level parallelism, threads are statically grouped into fixed-size independently scheduled warps. DITVA leverages existing SIMD units and maintains binary compatibility with existing CPU architectures
future=
comment=evaluation on the SPMD applications from the PARSEC and Rodinia OpenMP benchmarks shows that a 4-warp × 4-lane 4-issue DITVA architecture with a realistic bank-interleaved cache achieves 1.55× higher performance than a 4-thread 4-issue SMT architecture with AVX instructions while fetching and issuing 51% fewer instructions, achieving an overall 24% energy reduction
other=
---
id=4
title=Cambricon-LLM: A Chiplet-Based Hybrid Architecture for On-Device Inference of 70B LLM
author=Zhongkai Yu, Tianshi Chen
journal=micro
year=2024
tags=Cambricon company, AI DSA, AI accelerator, Cambricon-LLM, Chiplet-Based hybrid architecture, on-device Inference, 70B LLM, edge device, data privacy, in-flash computing, robotic accelerator
star=****
problem=Deploying advanced large language models on edge devices, such as smartphones and robotics, is a growing trend that enhances user data privacy and network connectivity resilience while preserving intelligent capabilities
interest=
hardness=such a task exhibits single-batch computing with incredibly low arithmetic intensity, which poses the significant challenges of huge memory footprint and bandwidth demands on limited edge resources
idea=Cambricon-LLM, a chiplet-based hybrid architecture with NPU and a dedicated NAND flash chip to enable efficient on-device inference of 70B LLMs. Such a hybrid architecture utilizes both the high comput-ing capability of NPU and the data capacity of the NAND flash chip, with the proposed hardware-tiling strategy that minimizes the data movement overhead between NPU and NAND flash chip. Specifically, the NAND flash chip, enhanced by our innovative in-flash computing and on-die ECC techniques, excels at performing precise lightweight on-die processing. Simultaneously, the NPU collaborates with the flash chip for matrix operations and handles special function computations beyond the flash’s on-die processing capabilities
future=
comment=Cambricon-LLM enables the on-device inference of 70B LLMs at a speed of 3.44 token/s, and 7B LLMs at a speed of 36.34 token/s, which is over 22×to 45× faster than existing flash-offloading technologies, show
other=
---
id=5
title=AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training
author=Zhenyu Han (Huawei), Ansheng You (PKU), Xin Huang (Huawei)
journal=arxiv
year=2025
tags=AsyncFlow, asynchronous streaming RL, MindSpeed-RL, LLM post-training
star=***
problem=
interest=
hardness=
idea=
future=
comment=
other=主要参考了streamrl和slime的设计，在昇腾上搞了一版，重在宣传站位，主要还是看代码啥时合入ms-rl才会有影响  ;实验部分极其地短，更像是个技术报告，实验不充分，看起来应该是在910C做的，而且主力模型是qwen稠密而没有考虑MoE
---
id=6
title=Accelerating Model Training on Ascend Chips: An Industrial System for Profiling, Analysis and Optimization
author=Yuhang Zhou
journal=USENIX ATC
year=2025
tags=model training, Ascend chip, Industrial system, profiling, analysis, optimization, Hermes, deep learning, LLM
star=****
problem=Training large-scale deep learning (DL) models is a resource-intensive and time-consuming endeavor, yet optimizing train-ing efficiency poses significant challenges
interest=The sporadic per-formance fluctuations during long training require advanced profiling capabilities
hardness=It is not easy to perform comprehen-sive and accurate bottleneck analysis amidst numerous in-fluencing factors. Selecting effective optimization strategies without proper guidance further complicates the process
idea=practical insights on optimizing training on Huawei Ascend chips based on three years of experience with 135 typical cases. We propose a systematic optimization system, Hermes, including a lightweight profiling approach, a hierarchical bottleneck analysis framework, and an optimization advisor
future=
comment=Our real-world experiments demonstrate significant acceleration in training for models like PanGu-α, MobileNetV1, and MoE (Mixture of Experts), with respective speedups of 3.05×, 1.91×, and 1.19×.
other=
---
id=7
title=MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism
author=Ruidong Zhu (ByteDance Seed), Xuanzhe Liu, Xin Jin (Peking University)
journal=SIGCOMM
year=2025
tags=MegaScale-Infer, MoE, Disaggregated expert Parallelism, LLM inference, FFN, GPU utilization, compute-intensive, memory-intensive, ping-pong pipeline parallelism, heterogeneous deployment, vLLM engine
star=****
problem=Mixture-of-Experts (MoE) showcases tremendous potential to scale large language models (LLMs) with enhanced performance and reduced computational complexity
interest=
hardness=its sparsely activated architecture shifts feed-forward networks (FFNs) from being compute-intensive to memory-intensive during inference, leading to substantially lower GPU utilization and increased operational costs
idea=MegaScale-Infer, an efficient and cost-effective system for serving large-scale MoE models. MegaScale-Infer disaggregates attention and FFN modules within each model layer, enabling independent scaling, tailored parallelism strategies, and heterogeneous deployment for both modules. To fully exploit disaggregation in the presence of MoE’s sparsity, MegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a request batch into micro-batches and shuttles them between attention and FFNs for inference. Combined with distinct model parallelism for each module, MegaScale-Infer effectively hides communication overhead and maximizes GPU utilization. To adapt to disaggregated attention and FFN modules and minimize data transmission overhead (e.g., token dispatch), MegaScale-Infer provides a high-performance M2N communication library that eliminates unnecessary GPU-to-CPU data copies, group initialization overhead, and GPU synchronization
future=
comment=Experimental results indicate that MegaScale-Infer achieves up to 1.90× higher per-GPU throughput than state-of-the-art solutions.
other=the former paper is DistServe [id:8] on dense model
---
id=8
title=DistServe: Disaggregating prefill and decoding for goodput-optimized large language model serving
author=Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu (StepFun), Xuanzhe Liu (pku), Xin Jin, Hao Zhang
journal=OSDI
year=2024
tags=LLM inference, DistServe, PD disaggregation, prefill phase, decode phase, goodput-optimized LLM serving, TTFT, TPOT
star=****
problem=DistServe improves the performance of large language models (LLMs) serving by disaggregating the prefill and decoding computation
interest=Existing LLM serving systems colocate the two phases and batch the computation of prefill and decoding across all users and requests
hardness=this strategy not only leads to strong prefill-decoding interferences but also couples the resource allocation and parallelism plans for both phases. LLM applications often emphasize individual latency for each phase: time to first token (TTFT) for the prefill phase and time per output token (TPOT) of each request for the decoding phase. In the presence of stringent latency requirements, existing systems have to prioritize one latency over the other, or over-provision compute resources to meet both
idea=DistServe assigns prefill and decoding computation to different GPUs, hence eliminating prefill-decoding interferences. Given the application's TTFT and TPOT requirements, DistServe co-optimizes the resource allocation and parallelism strategy tailored for each phase. DistServe also places the two phases according to the serving cluster's bandwidth to minimize the communication caused by disaggregation. As a result, DistServe significantly improves LLM serving performance in terms of the maximum rate that can be served within both TTFT and TPOT constraints on each GPU.
future=
comment=on various popular LLMs, applications, and latency requirements, DistServe can serve 7.4× more requests or 12.6× tighter SLO, compared to state-of-the-art systems, while staying within latency constraints for > 90% of requests.
other=on dense model, first introduce disaggregation of prefill and decode, used by NV and deepseek ; github.com/LLMServe/DistServe ; the moe version is [id:7]
---
id=9
title=Optimizing RLHF Training for Large Language Models with Stage Fusion
author=Yinmin Zhong, Yibo Zhu (StepFun), Xin Jin (pku)
journal=NSDI
year=2025
tags=RLHFuse, RLHF training, LLM, stage fusion, RL acceleration, data skewness, generation stage, pipeline bubble, training stage, GPU utilization, Megatron-LM, colocated architecture
star=****
problem=We present RLHFuse, an efficient training system with stage fusion for Reinforcement Learning from Human Feedback (RLHF)
interest=
hardness=Due to the intrinsic nature of RLHF training, i.e., the data skewness in the generation stage and the pipeline bubbles in the training stage, existing RLHF systems suffer from low GPU utilization
idea=RLHFuse breaks the traditional view of RLHF workflow as a composition of individual tasks, splitting each task into finer-grained subtasks, and performing stage fusion to improve GPU utilization. RLHFuse contains two key ideas. First, for generation and inference tasks, RLHFuse splits them into sample-level subtasks, enabling efficient inter-stage fusion to overlap the execution of generation and inference stages, thus mitigating the original generation bottleneck dominated by long-tailed samples. Second, for training tasks, RLHFuse breaks them into subtasks of micro-batches and performs intra-stage fusion to concurrently execute these subtasks in the training stage with a fused pipeline schedule, effectively mitigating the pipeline bubbles. 
future=
comment=The experiments show that RLHFuse increases the training throughput by up to 3.7×, compared to existing systems; still based on colocated architecture, but optimize a lot by stage fusion
other=implement RLHFuse based on Megatron-LM with 7K lines of code in Python, C++, and CUDA. Megatron-LM applies 3D-parallelism for single-model training. We extend it to support multiple device meshes to launch different tasks asynchronously with tailored parallelism and deployment strategy
---
id=10
title=StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation
author=Yinmin Zhong (pku), Zili Zhang, Yibo Zhu (StepFun), Daxin Jiang (StepFun)
journal=arXiv
year=2025
tags=StreamRL, Heterogeneous and elastic RL system, LLM post-training, Disaggregated stream Generation, colocated architecture, Disaggregated architecture, resource coupling, cross-datacenter, pipeline bubble, skewness bubble, Disaggregated RL framework, cost-effectiveness, long-tail output length distribution, dynamic resource scheduling
star=****
problem=Reinforcement learning (RL) has become the core post-training technique for large language models (LLMs)
interest=The LLM first generates samples online, which are then used to derive rewards for training; The conventional view holds that the colocated architecture—where the two stages share resources via temporal multiplexing—outperforms the disaggregated architecture, in which dedicated resources are assigned to each stage
hardness=in real-world deployments, we observe that the colocated architecture suffers from resource coupling, where the two stages are constrained to use the same resources. This coupling compromises the scalability and cost-efficiency of colocated RL in large scale training. In contrast, the disaggregated architecture allows for flexible resource allocation, supports heterogeneous training setups, and facilitates cross-datacenter deployment
idea=StreamRL is designed with disaggregation from first principles and fully unlocks its potential by addressing two types of performance bottlenecks in existing disaggregated RL frameworks: pipeline bubbles, caused by stage dependencies, and skewness bubbles, resulting from long-tail output length distributions. To address pipeline bubbles, StreamRL breaks the traditional stage boundary in synchronous RL algorithms through stream generation, and achieves fully overlapping in asynchronous RL. To address skewness bubbles, StreamRL employs an output-length ranker model to identify long-tail samples and reduces generation time via skewness-aware dispatching and scheduling (dynamically adjusting the resource of training and inference instances). 
future=
comment=Experiments show that StreamRL improves throughput by up to 2.66× compared to existing state-of-the-art systems, and improves cost-effectiveness by up to 1.33× in heterogeneous, cross-datacenter setting; compared to [id:9], further optimize by disaggregated architecture
other=SGS employs an in-house inference engine implemented in C++ with optimized CUDA kernels, supporting continuous batching  to release shorter samples early and prefix sharing  to save key-value cache usage. Trainer implements 3D parallelism similar to prior work. To address GPU memory constraints, we develop dynamic CPU offloading that interleaves the execution of different models through memory swapping
---
id=11
title=Serving Large Language Models on Huawei CloudMatrix384
author=Pengfei Zup, juncheng Liu (SiliconFlow), Zhou Yu, Heng Liao
journal=arXiv
year=2025
tags=LLM serving, Huawei CloudMatrix384, 910C chip, A3 server, Huawei cloud, AI infrastructure, super node, MoE, inference, inter-chip communication, co-designed hardware and software stack, unified bus (UB), Ascend NPU, Kunpeng CPU, all-to-all communication, key-value cache (KVcache), CloudMatrix-Infer, DeepSeek-R1, large-scale expert parallelism (EP), hardware-aware optimization
star=***
problem=The rapid evolution of large language models (LLMs), driven by increasing parameter scales, adoption of mixture-of-experts (MoE) architectures, and expanding context lengths, imposes unprecedented demands on AI infrastructure.
interest=
hardness=Conventional AI clusters are increasingly constrained by compute intensity, memory bandwidth limitations, inter-chip communication overhead, and stringent latency requirements. In real-world deployments, these challenges are further compounded by the need to handle diverse, bursty workloads, variable-length inputs, and imbalanced expert activations, while meeting strict service-level objectives. Overcoming these constraints requires a fundamentally re-architected, co-designed hardware and software stack.
idea=this paper introduces Huawei CloudMatrix, a next-generation AI datacen- ter architecture that embodies Huawei’s vision for reshaping the foundation of AI infrastructure. Huawei CloudMatrix384 represents the first production-grade realization of this vision. It integrates 384 Ascend 910 NPUs, 192 Kunpeng CPUs, and other hardware components into a unified supernode, interconnected via an ultra-high-bandwidth, low-latency Unified Bus (UB) network. Unlike conventional hierarchical designs, this architecture enables direct all-to-all communication via UB, allowing compute, memory, and network resources to be dynamically pooled, uniformly accessed, and independently scaled. These architectural features are particularly beneficial for communication-intensive operations such as large-scale MoE expert parallelism and distributed key-value (KV) cache access, making CloudMatrix384 a scalable and high-performance foundation for next-generation LLM serving. To fully harness CloudMatrix384’s capabilities, we propose CloudMatrix-Infer, a comprehensive LLM serving solution that establishes a best practice for deploying large-scale MoE models such as DeepSeek-R1.  CloudMatrix-Infer incorporates three core innovations. First, we design a peer-to-peer serving architecture that disaggregates prefill, decode, and caching into independently scalable resource pools. Unlike existing KV cache- centric architectures, this design enables high-bandwidth, uniform access to cached data via the UB network, thus reducing data locality constraints, simplifying task scheduling, and improving cache efficiency. Second, we design a large-scale expert parallelism (EP) strategy that leverages the UB network to achieve efficient token dispatch and expert output combination. This strategy supports a very large EP degree, e.g., EP320, enabling each NPU die to host exactly one expert, thus achieving low decode latency. Finally, we propose a set of hardware-aware optimizations tailored to CloudMatrix384, including highly-optimized operators, microbatch-based pipelining, and INT8 quantization, to enhance execution efficiency and resource utilization.
future=
comment=Our extensive evaluation with the DeepSeek-R1 model shows that CloudMatrix-Infer achieves state-of-the-art efficiency without sacrificing accuracy. CloudMatrix-Infer delivers a prefill throughput of 6,688 tokens/s per NPU, and a decode throughput of 1,943 tokens/s per NPU (at <50 ms TPOT). These results correspond to compute efficiencies of 4.45 tokens/s/TFLOPS for prefill and 1.29 tokens/s/TFLOPS for decode, both exceeding published results for SGLang on NVIDIA H100 and DeepSeek on NVIDIA H800. CloudMatrix-Infer also effectively manages the throughput-latency trade-off, sustaining a decode throughput of 538 tokens/s per NPU even under the stricter sub-15 ms TPOT constraint. Furthermore, the INT8 quantization on Ascend 910 maintains model accuracy comparable to the official DeepSeek-R1 API across 16 distinct benchmarks
other=
---
