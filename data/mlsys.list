---
id=
title=
author=
journal=
year=
tags=
star=
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=0
title=SKYLADDER: BETTER AND FASTER PRETRAINING VIA CONTEXT WINDOW SCHEDULING
author=Tongyao Zhu, Qian Liu, Haonan Wang, Shiqi Chen, Xiangming Gu, Tianyu Pang, Min-Yen Kan
journal=arXiv
year=2025
tags=MLsys, LLM infra, SKYLADDER, pretraining, dense model, context window schedule, long sequence
star=***
problem=models pretrained with shorter context windows consistently outperform their long-context counterparts under a fixed token budget
interest=Recent advancements in LLM pretraining have featured ever-expanding context windows to process longer sequences
hardness=long context not only harm the model performance, but also has higher traning costs
idea=explore an optimal context window scheduling strategy to better balance long-context capability with pretraining efficiency; SkyLadder, a simple yet effective approach that implements a short-to-long context window transition.
future=more dynamic adaptive scheduling for different model size and datasets, and MoE models, continue training and RL tasks, loss-based cutting
comment=SkyLadder preserves strong standard benchmark performance, while matching or exceeding baseline results on long-context tasks. Through extensive experiments, we pre-train 1B-parameter models (up to 32K context) and 3B-parameter models(8K context) on 100B tokens, demonstrating that SkyLadder yields consistent gains of up to 3.7% on common benchmarks, while achieving up to 22% faster training speeds compared to baselines
other=https://github.com/sail-sg/SkyLadder
---
id=1
title=Mirage: A Multi-Level Superoptimizer for Tensor Programs
author=Mengdi Wu, Xinhao Cheng, Shengyu Liu, Chunan Shi, Jianan Ji, Man Kit Ao, Praveen Velliengiri, Xupeng Miao, Oded Padon, Zhihao Jia
journal=OSDI
year=2025
tags=optimizer, tensor program, Mirage Persistent Kernel (MPK), LLM compiler, automated kernel generation, MegaKernel, uGraph, GPU compute hierarchy, compute graph, hierarchical graph representation, algebraic transformation, schedule transformation
star=****
problem=how to automatically generate big kernel containing all operations including communication
interest=codesign of algorithm and schedule; an algorithm describes what to compute in a kernel and a schedule specifies how to compute the kernel; Mirage automatically discovers and verifies sophisticated optimizations of tensor programs that require joint optimization of algebraic transformations, schedule transformations, and the discovery of new custom kernels
hardness=optimizes the schedule of a tensor program while fixing the algorithm. For a given algorithm, these optimizers automatically generate performant kernels by searching for possible strategies to execute the kernel on the target hardware. However, due to the linear algebra nature of DNNs, a tensor program can be represented by a wide spectrum of mathematically equivalent algorithms. Existing schedule-based optimizers only consider kernels whose algorithms are manually specified by users, resulting in missed optimization opportunities
idea=A key idea in Mirage is µGraphs, a uniform representation of tensor programs at the kernel, thread block, and thread levels of the GPU compute hierarchy. µGraphs enable Mirage to discover novel optimizations that combine algebraic transformations, schedule transformations, and generation of new custom kernels. To navigate the large search space, Mirage introduces a pruning technique based on abstraction that significantly reduces the search space and provides a certain optimality guarantee. To ensure that the optimized µGraph is equivalent to the input program, Mirage introduces a probabilistic equivalence verification procedure with strong theoretical guarantees
future=
comment= Mirage outperforms existing tensor program optimizers by up to 3.3×, even for widely used and heavily optimized DNNs ;  currently not support MoE well; MegaKernel of stanford is manually written for 1B model and single card
other=https://github.com/mirage-project/mirage/tree/mpk  ;  https://www.zhihu.com/question/1927927257713849225/answer/1928107746865185836?share_code=gjzLEtbuTe97&utm_psn=1930943423210329801
---
id=2
title=Task-Based Tensor Computations on Modern GPUs
author=ROHAN YADAV, MICHAEL GARLAND, ALEX AIKEN, MICHAEL BAUER
journal=PLDI
year=2025
tags=NVIDIA Hopper GPU, task-based tensor Computation, fixed-function unit, domain-specific, asynchronous data movement unit (TMA), asynchronous matrix multiplication unit (tensor core), warp-specialized kernel, producer-consumer pipeline, Cypress, programming model, compiler architecture, CUDA program
star=****
problem=Domain-specific, fixed-function units are becoming increasingly common in modern processors
interest=As the computational demands of applications evolve, the capabilities and programming interfaces of these fixed-function units continue to change. NVIDIA’s Hopper GPU architecture contains multiple fixed-function units per compute unit, including an asynchronous data movement unit (TMA) and an asynchronous matrix multiplication unit (Tensor Core).
hardness=Efficiently utilizing these units requires a fundamentally different programming style than previous architectures; programmers must now develop warp-specialized kernels that orchestrate producerconsumer pipelines between the asynchronous units
idea=To manage the complexity of programming these new architectures, we introduce Cypress, a task-based programming model with sequential semantics. Cypress programs are a set of designated functions called tasks that operate on tensors and are free of communication and synchronization. Cypress programs are bound to the target machine through a mapping specification that describes where tasks should run and in which memories tensors should be materialized. We present a compiler architecture that lowers Cypress programs into CUDA programs that perform competitively with expert-written codes
future=
comment=Cypress achieves 0.88x-1.06x the performance of cuBLAS on GEMM, and between 0.80x-0.98x the performance of the currently best-known Flash Attention implementation while eliminating all aspects of explicit data movement and asynchronous computation from application code
other=
---
id=3
title=Dynamic Inter-Thread Vectorization Architecture: extracting DLP from TLP
author=Sajith Kalathingal, Caroline Collange, Bharath Narasimha Swamy, André Seznec, Bharath N Swamy
journal=SBAC-PAD
year=2016
tags=DITVA, SPMD, DLP, TLP, SMT Architecture, AVX instruction, dynamic vector instruction
star=***
problem=generalize the successful SIMT GPU execution model to general-purpose SMT CPUs
interest=Threads of Single-Program Multiple-Data (SPMD) applications often execute the same instructions on different data.
hardness=
idea=propose the Dynamic Inter-Thread Vectorization Architecture (DITVA) to leverage this implicit data-level parallelism in SPMD applications by assembling dynamic vector instructions at runtime. DITVA extends an SIMD-enabled in-order SMT processor with an inter-thread vectorization execution mode. In this mode, multiple scalar threads running in lockstep share a single instruction stream and their respective instruction instances are aggregated into SIMD instructions. To balance thread- and data-level parallelism, threads are statically grouped into fixed-size independently scheduled warps. DITVA leverages existing SIMD units and maintains binary compatibility with existing CPU architectures
future=
comment=evaluation on the SPMD applications from the PARSEC and Rodinia OpenMP benchmarks shows that a 4-warp × 4-lane 4-issue DITVA architecture with a realistic bank-interleaved cache achieves 1.55× higher performance than a 4-thread 4-issue SMT architecture with AVX instructions while fetching and issuing 51% fewer instructions, achieving an overall 24% energy reduction
other=
---
id=4
title=Cambricon-LLM: A Chiplet-Based Hybrid Architecture for On-Device Inference of 70B LLM
author=Zhongkai Yu, Tianshi Chen
journal=micro
year=2024
tags=Cambricon company, AI DSA, AI accelerator, Cambricon-LLM, Chiplet-Based hybrid architecture, on-device Inference, 70B LLM, edge device, data privacy, in-flash computing, robotic accelerator
star=****
problem=Deploying advanced large language models on edge devices, such as smartphones and robotics, is a growing trend that enhances user data privacy and network connectivity resilience while preserving intelligent capabilities
interest=
hardness=such a task exhibits single-batch computing with incredibly low arithmetic intensity, which poses the significant challenges of huge memory footprint and bandwidth demands on limited edge resources
idea=Cambricon-LLM, a chiplet-based hybrid architecture with NPU and a dedicated NAND flash chip to enable efficient on-device inference of 70B LLMs. Such a hybrid architecture utilizes both the high comput-ing capability of NPU and the data capacity of the NAND flash chip, with the proposed hardware-tiling strategy that minimizes the data movement overhead between NPU and NAND flash chip. Specifically, the NAND flash chip, enhanced by our innovative in-flash computing and on-die ECC techniques, excels at performing precise lightweight on-die processing. Simultaneously, the NPU collaborates with the flash chip for matrix operations and handles special function computations beyond the flash’s on-die processing capabilities
future=
comment=Cambricon-LLM enables the on-device inference of 70B LLMs at a speed of 3.44 token/s, and 7B LLMs at a speed of 36.34 token/s, which is over 22×to 45× faster than existing flash-offloading technologies, show
other=
---
id=5
title=AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training
author=Zhenyu Han (Huawei), Ansheng You (PKU), Xin Huang (Huawei)
journal=arxiv
year=2025
tags=AsyncFlow, asynchronous streaming RL, MindSpeed-RL, LLM post-training, Huawei Ascend chip
star=***
problem=
interest=
hardness=
idea=
future=
comment=
other=主要参考了streamrl和slime的设计，在昇腾上搞了一版，重在宣传站位，主要还是看代码啥时合入ms-rl才会有影响  ;实验部分极其地短，更像是个技术报告，实验不充分，看起来应该是在910C做的，而且主力模型是qwen稠密而没有考虑MoE
---
id=6
title=Accelerating Model Training on Ascend Chips: An Industrial System for Profiling, Analysis and Optimization
author=Yuhang Zhou
journal=USENIX ATC
year=2025
tags=model training, Huawei Ascend chip, Industrial system, profiling, analysis, optimization, Hermes, deep learning, LLM
star=****
problem=Training large-scale deep learning (DL) models is a resource-intensive and time-consuming endeavor, yet optimizing train-ing efficiency poses significant challenges
interest=The sporadic per-formance fluctuations during long training require advanced profiling capabilities
hardness=It is not easy to perform comprehen-sive and accurate bottleneck analysis amidst numerous in-fluencing factors. Selecting effective optimization strategies without proper guidance further complicates the process
idea=practical insights on optimizing training on Huawei Ascend chips based on three years of experience with 135 typical cases. We propose a systematic optimization system, Hermes, including a lightweight profiling approach, a hierarchical bottleneck analysis framework, and an optimization advisor
future=
comment=Our real-world experiments demonstrate significant acceleration in training for models like PanGu-α, MobileNetV1, and MoE (Mixture of Experts), with respective speedups of 3.05×, 1.91×, and 1.19×.
other=
---
id=7
title=MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism
author=Ruidong Zhu (ByteDance Seed), Xuanzhe Liu, Xin Jin (Peking University)
journal=SIGCOMM
year=2025
tags=MegaScale-Infer, MoE, Disaggregated expert Parallelism, LLM inference, FFN, GPU utilization, compute-intensive, memory-intensive, ping-pong pipeline parallelism, heterogeneous deployment, vLLM engine
star=****
problem=Mixture-of-Experts (MoE) showcases tremendous potential to scale large language models (LLMs) with enhanced performance and reduced computational complexity
interest=
hardness=its sparsely activated architecture shifts feed-forward networks (FFNs) from being compute-intensive to memory-intensive during inference, leading to substantially lower GPU utilization and increased operational costs
idea=MegaScale-Infer, an efficient and cost-effective system for serving large-scale MoE models. MegaScale-Infer disaggregates attention and FFN modules within each model layer, enabling independent scaling, tailored parallelism strategies, and heterogeneous deployment for both modules. To fully exploit disaggregation in the presence of MoE’s sparsity, MegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a request batch into micro-batches and shuttles them between attention and FFNs for inference. Combined with distinct model parallelism for each module, MegaScale-Infer effectively hides communication overhead and maximizes GPU utilization. To adapt to disaggregated attention and FFN modules and minimize data transmission overhead (e.g., token dispatch), MegaScale-Infer provides a high-performance M2N communication library that eliminates unnecessary GPU-to-CPU data copies, group initialization overhead, and GPU synchronization
future=
comment=Experimental results indicate that MegaScale-Infer achieves up to 1.90× higher per-GPU throughput than state-of-the-art solutions.
other=the former paper is DistServe [id:8] on dense model
---
id=8
title=DistServe: Disaggregating prefill and decoding for goodput-optimized large language model serving
author=Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu (StepFun), Xuanzhe Liu (pku), Xin Jin, Hao Zhang
journal=OSDI
year=2024
tags=LLM inference, DistServe, PD disaggregation, prefill phase, decode phase, goodput-optimized LLM serving, TTFT, TPOT
star=****
problem=DistServe improves the performance of large language models (LLMs) serving by disaggregating the prefill and decoding computation
interest=Existing LLM serving systems colocate the two phases and batch the computation of prefill and decoding across all users and requests
hardness=this strategy not only leads to strong prefill-decoding interferences but also couples the resource allocation and parallelism plans for both phases. LLM applications often emphasize individual latency for each phase: time to first token (TTFT) for the prefill phase and time per output token (TPOT) of each request for the decoding phase. In the presence of stringent latency requirements, existing systems have to prioritize one latency over the other, or over-provision compute resources to meet both
idea=DistServe assigns prefill and decoding computation to different GPUs, hence eliminating prefill-decoding interferences. Given the application's TTFT and TPOT requirements, DistServe co-optimizes the resource allocation and parallelism strategy tailored for each phase. DistServe also places the two phases according to the serving cluster's bandwidth to minimize the communication caused by disaggregation. As a result, DistServe significantly improves LLM serving performance in terms of the maximum rate that can be served within both TTFT and TPOT constraints on each GPU.
future=
comment=on various popular LLMs, applications, and latency requirements, DistServe can serve 7.4× more requests or 12.6× tighter SLO, compared to state-of-the-art systems, while staying within latency constraints for > 90% of requests.
other=on dense model, first introduce disaggregation of prefill and decode, used by NV and deepseek ; github.com/LLMServe/DistServe ; the moe version is [id:7]
---
id=9
title=Optimizing RLHF Training for Large Language Models with Stage Fusion
author=Yinmin Zhong, Yibo Zhu (StepFun), Xin Jin (pku)
journal=NSDI
year=2025
tags=RLHFuse, RLHF training, LLM, stage fusion, RL acceleration, data skewness, generation stage, pipeline bubble, training stage, GPU utilization, Megatron-LM, colocated architecture
star=****
problem=We present RLHFuse, an efficient training system with stage fusion for Reinforcement Learning from Human Feedback (RLHF)
interest=
hardness=Due to the intrinsic nature of RLHF training, i.e., the data skewness in the generation stage and the pipeline bubbles in the training stage, existing RLHF systems suffer from low GPU utilization
idea=RLHFuse breaks the traditional view of RLHF workflow as a composition of individual tasks, splitting each task into finer-grained subtasks, and performing stage fusion to improve GPU utilization. RLHFuse contains two key ideas. First, for generation and inference tasks, RLHFuse splits them into sample-level subtasks, enabling efficient inter-stage fusion to overlap the execution of generation and inference stages, thus mitigating the original generation bottleneck dominated by long-tailed samples. Second, for training tasks, RLHFuse breaks them into subtasks of micro-batches and performs intra-stage fusion to concurrently execute these subtasks in the training stage with a fused pipeline schedule, effectively mitigating the pipeline bubbles. 
future=
comment=The experiments show that RLHFuse increases the training throughput by up to 3.7×, compared to existing systems; still based on colocated architecture, but optimize a lot by stage fusion ; only use LLama dense models
other=implement RLHFuse based on Megatron-LM with 7K lines of code in Python, C++, and CUDA. Megatron-LM applies 3D-parallelism for single-model training. We extend it to support multiple device meshes to launch different tasks asynchronously with tailored parallelism and deployment strategy
---
id=10
title=StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation
author=Yinmin Zhong (pku), Zili Zhang, Yibo Zhu (StepFun), Daxin Jiang (StepFun)
journal=arXiv
year=2025
tags=StreamRL, Heterogeneous and elastic RL system, LLM post-training, Disaggregated stream Generation, colocated architecture, Disaggregated architecture, resource coupling, cross-datacenter, pipeline bubble, skewness bubble, Disaggregated RL framework, cost-effectiveness, long-tail output length distribution, dynamic resource scheduling
star=****
problem=Reinforcement learning (RL) has become the core post-training technique for large language models (LLMs)
interest=The LLM first generates samples online, which are then used to derive rewards for training; The conventional view holds that the colocated architecture—where the two stages share resources via temporal multiplexing—outperforms the disaggregated architecture, in which dedicated resources are assigned to each stage
hardness=in real-world deployments, we observe that the colocated architecture suffers from resource coupling, where the two stages are constrained to use the same resources. This coupling compromises the scalability and cost-efficiency of colocated RL in large scale training. In contrast, the disaggregated architecture allows for flexible resource allocation, supports heterogeneous training setups, and facilitates cross-datacenter deployment
idea=StreamRL is designed with disaggregation from first principles and fully unlocks its potential by addressing two types of performance bottlenecks in existing disaggregated RL frameworks: pipeline bubbles, caused by stage dependencies, and skewness bubbles, resulting from long-tail output length distributions. To address pipeline bubbles, StreamRL breaks the traditional stage boundary in synchronous RL algorithms through stream generation, and achieves fully overlapping in asynchronous RL. To address skewness bubbles, StreamRL employs an output-length ranker model to identify long-tail samples and reduces generation time via skewness-aware dispatching and scheduling (dynamically adjusting the resource of training and inference instances). 
future=
comment=Experiments show that StreamRL improves throughput by up to 2.66× compared to existing state-of-the-art systems, and improves cost-effectiveness by up to 1.33× in heterogeneous, cross-datacenter setting; compared to [id:9], further optimize by disaggregated architecture ; only use Qwen2.5 dense models
other=SGS employs an in-house inference engine implemented in C++ with optimized CUDA kernels, supporting continuous batching  to release shorter samples early and prefix sharing  to save key-value cache usage. Trainer implements 3D parallelism similar to prior work. To address GPU memory constraints, we develop dynamic CPU offloading that interleaves the execution of different models through memory swapping
---
id=11
title=Serving Large Language Models on Huawei CloudMatrix384
author=Pengfei Zup, juncheng Liu (SiliconFlow), Zhou Yu, Heng Liao
journal=arXiv
year=2025
tags=LLM serving, Huawei CloudMatrix384, 910C chip, A3 server, Huawei cloud, AI infrastructure, super node, MoE, inference, inter-chip communication, co-designed hardware and software stack, unified bus (UB), Ascend NPU, Kunpeng CPU, all-to-all communication, key-value cache (KVcache), CloudMatrix-Infer, DeepSeek-R1, large-scale expert parallelism (EP), hardware-aware optimization
star=***
problem=The rapid evolution of large language models (LLMs), driven by increasing parameter scales, adoption of mixture-of-experts (MoE) architectures, and expanding context lengths, imposes unprecedented demands on AI infrastructure.
interest=
hardness=Conventional AI clusters are increasingly constrained by compute intensity, memory bandwidth limitations, inter-chip communication overhead, and stringent latency requirements. In real-world deployments, these challenges are further compounded by the need to handle diverse, bursty workloads, variable-length inputs, and imbalanced expert activations, while meeting strict service-level objectives. Overcoming these constraints requires a fundamentally re-architected, co-designed hardware and software stack.
idea=this paper introduces Huawei CloudMatrix, a next-generation AI datacen- ter architecture that embodies Huawei’s vision for reshaping the foundation of AI infrastructure. Huawei CloudMatrix384 represents the first production-grade realization of this vision. It integrates 384 Ascend 910 NPUs, 192 Kunpeng CPUs, and other hardware components into a unified supernode, interconnected via an ultra-high-bandwidth, low-latency Unified Bus (UB) network. Unlike conventional hierarchical designs, this architecture enables direct all-to-all communication via UB, allowing compute, memory, and network resources to be dynamically pooled, uniformly accessed, and independently scaled. These architectural features are particularly beneficial for communication-intensive operations such as large-scale MoE expert parallelism and distributed key-value (KV) cache access, making CloudMatrix384 a scalable and high-performance foundation for next-generation LLM serving. To fully harness CloudMatrix384’s capabilities, we propose CloudMatrix-Infer, a comprehensive LLM serving solution that establishes a best practice for deploying large-scale MoE models such as DeepSeek-R1.  CloudMatrix-Infer incorporates three core innovations. First, we design a peer-to-peer serving architecture that disaggregates prefill, decode, and caching into independently scalable resource pools. Unlike existing KV cache- centric architectures, this design enables high-bandwidth, uniform access to cached data via the UB network, thus reducing data locality constraints, simplifying task scheduling, and improving cache efficiency. Second, we design a large-scale expert parallelism (EP) strategy that leverages the UB network to achieve efficient token dispatch and expert output combination. This strategy supports a very large EP degree, e.g., EP320, enabling each NPU die to host exactly one expert, thus achieving low decode latency. Finally, we propose a set of hardware-aware optimizations tailored to CloudMatrix384, including highly-optimized operators, microbatch-based pipelining, and INT8 quantization, to enhance execution efficiency and resource utilization.
future=
comment=Our extensive evaluation with the DeepSeek-R1 model shows that CloudMatrix-Infer achieves state-of-the-art efficiency without sacrificing accuracy. CloudMatrix-Infer delivers a prefill throughput of 6,688 tokens/s per NPU, and a decode throughput of 1,943 tokens/s per NPU (at <50 ms TPOT). These results correspond to compute efficiencies of 4.45 tokens/s/TFLOPS for prefill and 1.29 tokens/s/TFLOPS for decode, both exceeding published results for SGLang on NVIDIA H100 and DeepSeek on NVIDIA H800. CloudMatrix-Infer also effectively manages the throughput-latency trade-off, sustaining a decode throughput of 538 tokens/s per NPU even under the stricter sub-15 ms TPOT constraint. Furthermore, the INT8 quantization on Ascend 910 maintains model accuracy comparable to the official DeepSeek-R1 API across 16 distinct benchmarks
other=
---
id=12
title=Splitwise: Efficient Generative LLM Inference Using Phase Splitting
author=Pratyush Patel, Esha Choukse, Chaojie Zhang
journal=ISCA
year=2024
tags=Splitwise, LLM inference, decoding phase, phase splitting, PD disaggregation, CPU offloading, homogeneous, heterogeneous
star=****
problem=Generative large language model (LLM) applications are growing rapidly, leading to large-scale deployments of expensive and power-hungry GPUs.
interest=Our characterization of LLM inference shows that each inference request undergoes two phases: a compute-intensive prompt computation phase and a memory intensive token generation phase, each with distinct latency, throughput, memory, and power characteristics. 
hardness=Despite state-of-the-art batching and scheduling, the token generation phase underutilizes compute resources. Unlike prompt computation, token generation does not need the compute capability of the latest GPUs and can be run with lower power and cost. 
idea=Based on these insights, we propose Splitwise, a model deployment and scheduling technique that splits the two phases of LLM inference requests on to separate machines. Splitwise enables phase-specific resource management using hardware that is well suited for each phase. Request state is transferred efficiently between machines using optimized network libraries on the fast back-plane interconnects available in today’s GPU clusters. 
future=
comment=Using Splitwise, we design homogeneous and heterogeneous LLM inference clusters optimized for throughput, cost, and power Compared to current designs, Splitwise clusters achieve up to 1.4× higher throughput at 20% lower cost. Alternatively, they can deliver 2.35× more throughput under the same power and cost budgets.
other=similar to DistServe, but at the same time
---
id=13
title=HELIOS: Harmonizing Early Fusion, Late Fusion, and LLM Reasoning for Multi-Granular Table-Text Retrieval
author=Sungho Park, Joohyung Yun, Jongwuk Lee, Wook-Shin Han
journal=ACL
year=2025
tags=HELIOS, early fusion, late fusion, LLM reasoning, Multi-Granular table-text Retrieval, open-domain question answering (ODQA), multi-hop reasoning, column-wise aggregation, edge-based bipartite subgraph Retrieval, query-relevant node expansion, top-k Retrieval, fused block, retrieval unit, evidence chain
star=****
problem=Table-text retrieval aims to retrieve relevant tables and text to support open-domain question answering
interest=The early fusion attempts to reduce the search space by grouping relevant documents before a query is presented; , the late fusion aligns relevant table rows and passages dynamically using query-based similarity matching after the query is given
hardness= Existing studies use either early or late fusion, but face limitations. Early fusion pre-aligns a table row with its associated passages, forming “stars," which often include irrelevant contexts and miss query-dependent relationships. Late fusion retrieves individual nodes, dynamically aligning them, but it risks missing relevant contexts. Both approaches also struggle with advanced reasoning tasks, such as column-wise aggregation and multihop reasoning
idea=HELIOS, which combines the strengths of both approaches. First, the edge-based bipartite subgraph retrieval identifies finer-grained edges between table segments and passages, effectively avoiding the inclusion of irrelevant contexts. Then, the query-relevant node expansion identifies the most promising nodes, dynamically retrieving relevant edges to grow the bipartite subgraph, minimizing the risk of missing important contexts. Lastly, the star-based LLM refinement performs logical inference at the star graph level rather than the bipartite subgraph, supporting advanced reasoning tasks 
future=
comment=Experimental results show that HELIOS outperforms state-of-the-art models with a significant improvement up to 42.6% and 39.9% in recall and nDCG, respectively, on the OTT-QA benchmark
other=
---
id=14
title=Optimizing SLO-oriented LLM Serving with PD-Multiplexing
author=Weihao Cui, Yukang Chen, Quan Chen, Shixuan Sun
journal=arXiv
year=2025
tags=DRIFT, SLO-oriented LLM serving, PD-Multiplexing, LLM inference, request scheduling, high throughput, multi-turn workflow, prefill phase, decode phase, out-of-place compute partition, in-place memory sharing, KV cache reuse, SLO-aware dispatching policy, spatial multiplexing
star=****
problem=Modern LLM services demand high throughput and stringent SLO guarantees across two distinct inference phases (prefill and decode) and complex multi-turn workflows
interest=
hardness= current systems face a fundamental tradeoff: out-ofplace compute partition enables per-phase SLO attainment, while in-place memory sharing maximizes throughput via KV cache reuse; existing in-place compute partition also encounters low utilization and high overhead due to phase-coupling design
idea=Drift, a new LLM serving framework that resolves this tension via PD multiplexing, enabling in-place and phase-decoupled compute partition. Drift leverages low-level GPU partitioning techniques to multiplex prefill and decode phases spatially and adaptively on shared GPUs, while preserving in-place memory sharing. To fully leverage the multiplexing capability, Drift introduces an adaptive gang scheduling mechanism, a contention-free modeling method, and a SLO-aware dispatching policy
future=
comment=Drift achieves an average 5.1× throughput improvement (up to 17.5×) over state-of-the-art baselines, while consistently meeting SLO targets under complex LLM workloads
other=
---
id=15
title=Principles and Methodologies for Serial Performance Optimization
author=Sujin Park, Mingyu Guan, Xiang Cheng, Taesoo Kim
journal=OSDI
year=2025
tags=Principle, structured methodology, systematic optimization, system performance, SysGPT, serial Performance optimization, AI4system, latency, throughput, task removal, replacement, reordering,  batching, caching, precomputing, deferring, relaxation, contextualization, hardware specialization, layering, optimization assistant, file and storage system, kernel synchronization, context-aware performance suggestion, sequential execution, Amdahl's law, survey, Experiment, insight
star=****
problem=Throughout the history of computer science, optimizing existing systems to achieve higher performance has been a longstanding aspiration
interest=The optimization of sequential execution is particularly crucial, as Amdahl’s law [8] highlights that the maximum potential speedup of a system is constrained by the fraction of the program that must remain sequential. Despite advances in parallel processing, addressing the sequential bottleneck remains foundational for achieving meaningful performance gains
hardness=While the primary emphasis of this endeavor lies in reducing latency and increasing throughput, these two are closely intertwined, and answering the how question has remained a challenge, often relying on intuition and experience
idea=This paper introduces a systematic approach to optimizing sequential tasks, which are fundamental for overall performance. We define three principles—task removal, replacement, and reordering—and distill them into eight actionable methodologies: batching, caching, precomputing, deferring, relaxation, contextualization, hardware specialization, and layering. Our review of OSDI and SOSP papers over the past decade shows that these techniques, when taken together, comprehensively account for the observed sequential optimization strategies.  To illustrate the framework’s practical value, we present two case studies: one on file and storage systems, and another analyzing kernel synchronization to uncover missed optimization opportunities. Furthermore, we introduce SysGPT, a fine-tuned GPT model trained on curated literature analysis, which offer context-aware performance suggestions
future=
comment=SysGPT’s outputs are more specific and feasible than GPT-4’s, aligning with core strategies from recent research without direct exposure, demonstrating its utility as an optimization assistant.
other=https://mp.weixin.qq.com/s/pvERUjVQbh6WIDheKNk3ZA?scene=1   ;   https://github.com/sslab-gatech/SysGPT
---
id=16
title=DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training
author=Zhixin Wang, Tianyi Zhou, Liming Liu, Ao Li, Jiarui Hu, Dian Yang, Jinlong Hou, Siyuan Feng (Shanghai innovation institute), Yuan Cheng, Yuan Qi
journal=arXiv
year=2025
tags=DistFlow, distributed RL framework, siiRL, LLM, multi-agent system, AI infra, post training, RL scaling, hybrid-controller architecture, single-controller, multi-controller, linear scalability
star=***
problem=Effectively scaling reinforcement learning is now the key to unlocking advanced reasoning capabilities and ensuring safe, goal-aligned behavior in the most powerful LLMs
interest=Reinforcement learning (RL) has become the pivotal posttraining technique for large language model
hardness=Mainstream frameworks usually employ a hybrid-controller architecture where a single-controller dispatches the overall execution logic and manages overall data transfer and the multi-controller executes distributed computation. For large-scale reinforcement learning, minor load imbalances can introduce significant bottlenecks, ultimately constraining the scalability of the system
idea=adopt a multi-controller paradigm that dispatches data transfer and execution tasks to all workers, which eliminates the centralized node. This allows each worker to operate independently, leading to near-linear scalability up to thousands of GPUs and dramatic efficiency gains. Furthermore, our architecture decouples resource configuration from execution logic, allowing each worker to have a unique execution flow, offering significant flexibility for rapid and cost-effective algorithmic experimentation
future=
comment=Extensive experiments show that DistFlow achieves excellent linear scalability and up to a 7x end-to-end throughput improvement over state-of-the-art (SOTA) frameworks.
other=https://github.com/sii-research/siiRL
---
id=17
title=TURBORAG: ACCELERATING RETRIEVAL-AUGMENTED GENERATION WITH PRECOMPUTED KV CACHES FOR CHUNKED TEXT
author=Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, Yaohua Tang
journal=arXiv
year=2024
tags=TurboRAG, RETRIEVAL-AUGMENTED generation, PRECOMPUTED KV cache, chunked text, lossy acceleration, prefill, TTFT, LLM inference, long context, cross-attention
star=***
problem=Current Retrieval-Augmented Generation (RAG) systems concatenate and process numerous retrieved document chunks for prefill which requires a large volume of computation, therefore leading to significant latency in time-to-first-token (TTFT).
interest=prefix cache only works for prompts with the same prefix, but how to utilize caches for middle chunks
hardness=how to ensure the model accuracy; First, repeatedly recalled document chunks require recomputation of the key-value (KV) caches, leading to redundant computation. Second, the augmented document contains substantially more tokens for prefill which contributes to considerably more computational overhead since the computation cost of KV caches is quadratic to the input sequence length.  Third, as a side effect of the requirement in substantial computation resources for concatenated document prefill, the batch size on a single device might be limited.
idea=To reduce the computation overhead as well as TTFT, we introduce TurboRAG, a novel RAG system that redesigns the inference paradigm of the current RAG system by first pre-computing and storing the key-value (KV) caches of documents offline, and then directly retrieving the saved KV cache for prefill. Hence, online computation of KV caches is eliminated during inference. In addition, we provide a number of insights into the mask matrix and positional embedding mechanisms, plus fine-tune a pretrained language model to maintain model accuracy of TurboRAG
future=
comment=Our approach is applicable to most existing large language models and their applications without any requirement in modification of models and inference systems. Experimental results across a suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x compared to the conventional RAG systems (on an average of 8.6x), but reserving comparable performance to the standard RAG systems.
other=https://github.com/MooreThreads/TurboRAG
---
id=18
title=Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification
author=Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaosheng Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, Yao Hu, Shaohui Lin
journal=arXiv
year=2025
tags=MLLM, Multi-modal, Dynamic-LLaVA, Multimodal large language model, dynamic Vision-language context Sparsification, prefill, decode
star=***
problem=
interest=
hardness=
idea=
future=
comment=
other=https://zhuanlan.zhihu.com/p/1898382148509074278    ;    https://github.com/Osilly/dynamic_llava
---
id=19
title=VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo
author=Qianli Ma (ByteDance)
journal=arXiv
year=2025
tags=VeOmni, multi-modal, any Modality model training, Model-Centric Distributed framework, omni-modal, heterogeneous model architecture, diverse Modality, flexibility, scalability, 3D parallelism, LLM, MoE, any-to-any, new modal as plugin, ByteDance
star=***
problem=Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation
interest= training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training
hardness=Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training
idea=VeOmni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. VeOmni introduces modelcentric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change
future=
comment= Using VeOmni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs
other= https://github.com/ByteDance-Seed/VeOmni  ; https://mp.weixin.qq.com/s/A1CdiEiSaGrh_aH_ggBINg?scene=1
---
id=20
title=iFairy: the First 2-bit Complex LLM with All Parameters in {±1, ±i}
author=Feiyu Wang, Guoan Wang, Yihao Zhang, Shengfan Wang, Weitao Li, Bokai Huang, Shimao Chen, Zihan Jiang, Rui Xu, Tong Yang
journal=arXiv
year=2025
tags=iFairy, 2-bit Complex LLM, complex domain computation, Fairy-plus-minus-i, Quantization-Aware Training (QAT), LLM inference, low-bit representation, full-precision accuracy, 2-bit representation, multiplication-free inference, addition, element swap
star=****
problem=Quantization-Aware Training (QAT) integrates quantization into the training loop, enabling LLMs to learn robust lowbit representations, and is widely recognized as one of the most promising research directions
interest=
hardness= All current QAT research focuses on minimizing quantization error on full-precision models, where the full-precision accuracy acts as an upper bound (accuracy ceiling). No existing method has even attempted to surpass this ceiling
idea=To break this ceiling, we propose a new paradigm: raising the ceiling (full-precision model), and then still quantizing it efficiently into 2 bits.  We propose iFairy, the first 2-bit quantization framework for complex-valued LLMs. Specifically, our method leverages the representational advantages of the complex domain to boost full-precision accuracy. We map weights to the fourth roots of unity {±1, ±i}, forming a perfectly symmetric and information-theoretically optimal 2-bit representation. Importantly, each quantized weight has either a zero real or imaginary part, enabling multiplication-free inference using only additions and element swaps
future=
comment=Experimental results show that iFairy outperforms the ceiling of existing 2-bit quantization approaches in terms of both PPL and downstream tasks, while maintaining strict storage and compute efficiency. This work opens a new direction for building highly accurate and practical LLMs under extremely low-bit constraints.
other=https://github.com/PKULab1806/Fairy-plus-minus-i
---
id=21
title=ByteCheckpoint: A Unified Checkpointing System for Large Foundation Model Development
author=Borui Wan (ByteDance)
journal=NSDI
year=2025
tags=ByteCheckpoint, Checkpointing system, LLM training, large Foundation model, ByteDance
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=https://github.com/ByteDance-Seed/ByteCheckpoint?tab=readme-ov-file
---
id=22
title=Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler
author=Size Zheng (ByteDance Seed)
journal=arXiv
year=2025
tags=Triton-distributed, Overlapping kernel, Distributed AI system, Triton compiler, LLM, ByteDance, multi-stream
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=https://github.com/ByteDance-Seed/Triton-distributed  ; https://zhuanlan.zhihu.com/p/1900910901017679250
---
id=23
title=Tilelink: Generating efficient compute-communication overlapping kernels using tile-centric primitives
author=Size Zheng (ByteDance Seed)
journal=arXiv
year=2025
tags=Tilelink, compute-communication overlapping kernel, tile-centric primitive, LLM, ByteDance
star=***
problem=
interest=
hardness=
idea=
future=
comment=
other=a type of implementation for Triton-distributed
---
