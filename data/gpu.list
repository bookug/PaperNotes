---
id=
title=
author=
journal=
year=
tags=
star=
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=0
title=Benchmarking GPUs to tune dense linear algebra
author=Volkov Vasily
journal=
year=2008
tags=nvidia gpu, dense linear algebra
star=***
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=1
title=Optimization principles and application performance evaluation of a multithreaded GPU using CUDA
author=Shane Ryoo
journal=PPoPP
year=2008
tags=GPU computing, CUDA, Optimization principles, lessons, design, performance evaluation, languages, parallel computing
star=****
problem=
interest=
hardness=
idea=developers face the challenge of striking the right balance between each thread's resource usage and the number of simultaneously active threads. The resources to manage include the number of registers and the amount of on-chip memory used per thread, number of threads per multiprocessor, and global memory bandwidth. We also obtain increased performance by reordering accesses to off-chip memory to combine requests to the same or contiguous memory locations and apply classical optimizations to reduce the number of executed operations
future=
comment=
other=
---
id=2
title=Selective GPU caches to eliminate CPU-GPU HW cache coherence
author=Neha Agarwal
journal=IEEE International Symposium on High Performance Computer Architecture (HPCA)
year=2016
tags=CPU-GPU HW cache coherence, gpu caches, strategies for cache coherence
star=****
problem=Recent work suggests extending hardware cache coherence between CPUs and GPUs to help support programming models with tightly coordinated sharing between CPU and GPU threads.
interest=Cache coherence is ubiquitous in shared memory multiprocessors because it provides a simple, high performance memory abstraction to programmers. 
hardness=implementing hardware cache coherence is particularly challenging in systems with discrete CPUs and GPUs that may not be produced by a single vendor
idea=selective caching, wherein we disallow GPU caching of any memory that would require coherence updates to propagate between the CPU and GPU, thereby decoupling the GPU from vendor-specific CPU coherence protocols. We propose several architectural improvements to offset the performance penalty of selective caching: aggressive request coalescing, CPU-side coherent caching for GPU-uncacheable requests, and a CPU-GPU interconnect optimization to support variable-size transfers. Moreover, current GPU workloads access many read-only memory pages; we exploit this property to allow promiscuous GPU caching of these pages, relying on page-level protection, rather than hardware cache coherence, to ensure correctness.
future=
comment=
other=
---
id=3
title=3D GPU architecture using cache stacking: Performance, cost, power and thermal analysis
author=Ahmed Al Maashri
journal=IEEE International Conference on Computer Design
year=2009
tags=3D GPU architecture, cache stacking, 3D die stacking, performance evaluation
star=***
problem=Graphics Processing Units (GPUs) offer tremendous computational and processing power. The architecture requires high communication bandwidth and lower latency between computation units and caches
interest=3D die-stacking technology is a promising approach to meet such requirements
hardness=
idea=the impact of stacking caches using the 3D technology on GPU performance. We also investigate the benefits of using 3D stacked MRAM on GPUs. Our work includes cost, power, and thermal analysis of the proposed architectural designs
future=
comment=
other=
---
id=4
title=Cache coherence for GPU architectures
author=Inderpreet Singh
journal=IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)
year=2013
tags=cache coherence, gpu Architecture
star=****
problem=Introducing conventional directory protocols adds unnecessary coherence traffic overhead to existing GPU applications. Moreover, these protocols increase the verification complexity of the GPU memory system
interest=While scalable coherence has been extensively studied in the context of general purpose chip multiprocessors (CMPs), GPU architectures present a new set of challenges
hardness=
idea=a time-based coherence framework for GPUs, called Temporal Coherence (TC), that exploits globally synchronized counters in single-chip systems to develop a streamlined GPU coherence protocol. Synchronized counters enable all coherence transitions, such as invalidation of cache blocks, to happen synchronously, eliminating all coherence traffic and protocol races. We present an implementation of TC, called TC-Weak, which eliminates LCC's trade-off between stalling stores and increasing L1 miss rates to improve performance and reduce interconnect traffic.
future=
comment=
other=
---
