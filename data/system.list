---
id=
title=
author=
journal=
year=
tags=
star=
problem=
interest=
hardness=
idea=
future=
comment=
other=
---
id=0
title= NanoLog: A Nanosecond Scale Logging System
author=Stephen Yang, Seo Jin Park, John Ousterhout (Stanford University)
journal=USENIX ATC
year=2018
tags=Nanolog, Logging system, high-performance, lock-free, low latency, high throughput
star=****
problem=NanoLog is a nanosecond scale logging system that is 1-2 orders of magnitude faster than existing logging systems such as Log4j2, spdlog, Boost log or Event Tracing for Windows. 
interest=The system achieves a throughput up to 80 million log messages per second for simple messages and has a typical log invocation overhead of 8 nanoseconds in microbenchmarks and 18 nanoseconds in applications, despite exposing a traditional printf-like API. 
hardness=
idea=NanoLog achieves this low latency and high throughput by shifting work out of the runtime hot path and into the compilation and post-execution phases of the application. More specifically, it slims down user log messages at compile-time by extracting static log components, outputs the log in a compacted, binary format at runtime, and utilizes an offline process to re-inflate the compacted logs. Additionally, log analytic applications can directly consume the compacted log and see a performance improvement of over 8x due to I/O savings. 
future=
comment=
other=https://zhuanlan.zhihu.com/p/136208506?utm_source=wechat_session&utm_medium=social&utm_oi=909929360004366336
---
id=1
title=A Generic Communication Scheduler for Distributed DNN Training Acceleration
author=Yanghua Peng, Yibo Zhu, Yangrui Chen, Yixin Bao, Bairen Yi, Chang Lan, Chuan Wu, Chuanxiong Guo
journal=SOSP (CCF conference of Software Engineering)
year=2019
tags=Communication Scheduler, Distributed DNN training, BytePS system, new AllReduce method, improved Parameter Server, data parallelism, model parallelism, bandwidth server,  pipeline design of ReduceScatter and AllGather, ByteScheduler
star=****
problem=
interest=
hardness=
idea=
future=
comment=
other=https://github.com/bytedance/byteps   https://www.zhihu.com/question/331936923   分布式训练的方案和效率对比 https://zhuanlan.zhihu.com/p/50116885    清华大学发布的自研深度学习框架-计图(Jittor)   https://www.zhihu.com/question/380993685/answer/1093243952    
---
